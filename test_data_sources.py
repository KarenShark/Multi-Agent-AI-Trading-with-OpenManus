#!/usr/bin/env python3
"""
æ•°æ®æºéªŒè¯å’Œè´¨é‡ç›‘æ§ç³»ç»Ÿ

éªŒè¯æ‰€æœ‰æ•°æ®æºçš„å¯ç”¨æ€§ã€è´¨é‡å’Œä¸€è‡´æ€§ï¼Œå»ºç«‹æŒç»­ç›‘æ§æœºåˆ¶ã€‚
åŒ…æ‹¬æ•°æ®å®Œæ•´æ€§ã€æ—¶æ•ˆæ€§ã€å‡†ç¡®æ€§å’Œå¯é æ€§çš„å…¨é¢æ£€æµ‹ã€‚
"""

import json
import os
import sys
import time
import traceback
from datetime import datetime, timedelta
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple

import numpy as np
import pandas as pd

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from app.data.loader import UnifiedDataLoader
from app.data.preprocess import DataPreprocessor
from app.tool.enhanced_news_fetcher import EnhancedNewsFetcher
from app.tool.fundamental_fetcher import FundamentalFetcher
from app.tool.macro_economic_fetcher import MacroEconomicFetcher
from app.tool.yfinance_fetcher import YFinanceFetcher


class DataSourceValidator:
    """æ•°æ®æºéªŒè¯å™¨ - å…¨é¢éªŒè¯æ•°æ®è´¨é‡å’Œå¯é æ€§"""

    def __init__(self):
        # åˆå§‹åŒ–æ‰€æœ‰æ•°æ®è·å–å™¨
        self.yfinance_fetcher = YFinanceFetcher()
        self.fundamental_fetcher = FundamentalFetcher()
        self.news_fetcher = EnhancedNewsFetcher()
        self.macro_fetcher = MacroEconomicFetcher()
        self.data_loader = UnifiedDataLoader()
        self.preprocessor = DataPreprocessor()

        # æµ‹è¯•è‚¡ç¥¨æ± 
        self.test_stocks = [
            "AAPL",
            "MSFT",
            "GOOGL",
            "AMZN",
            "TSLA",
            "META",
            "NVDA",
            "JPM",
            "V",
            "JNJ",
        ]

        # æ•°æ®è´¨é‡æ ‡å‡†
        self.quality_standards = {
            "minimum_data_points": 10,
            "maximum_age_days": 7,
            "minimum_coverage_ratio": 0.7,
            "maximum_missing_ratio": 0.3,
            "minimum_data_variance": 0.001,
            "consistency_tolerance": 0.05,
        }

        # éªŒè¯ç»“æœ
        self.validation_results = {
            "timestamp": datetime.now().isoformat(),
            "overall_status": "unknown",
            "data_sources": {},
            "quality_metrics": {},
            "issues_found": [],
            "recommendations": [],
        }

    def run_comprehensive_validation(self) -> Dict:
        """è¿è¡Œå…¨é¢çš„æ•°æ®æºéªŒè¯"""
        print("ğŸ” å¼€å§‹æ•°æ®æºå…¨é¢éªŒè¯")
        print("=" * 60)

        try:
            # 1. åŸºç¡€æ•°æ®æºè¿æ¥æµ‹è¯•
            print("\nğŸ“¡ æµ‹è¯•æ•°æ®æºè¿æ¥æ€§...")
            self._test_data_source_connectivity()

            # 2. æ•°æ®è´¨é‡éªŒè¯
            print("\nğŸ“Š éªŒè¯æ•°æ®è´¨é‡...")
            self._validate_data_quality()

            # 3. æ•°æ®ä¸€è‡´æ€§æ£€æŸ¥
            print("\nğŸ”„ æ£€æŸ¥æ•°æ®ä¸€è‡´æ€§...")
            self._check_data_consistency()

            # 4. æ€§èƒ½å’Œå“åº”æ—¶é—´æµ‹è¯•
            print("\nâš¡ æµ‹è¯•æ€§èƒ½å’Œå“åº”æ—¶é—´...")
            self._test_performance()

            # 5. æ•°æ®å®Œæ•´æ€§éªŒè¯
            print("\nâœ… éªŒè¯æ•°æ®å®Œæ•´æ€§...")
            self._validate_data_completeness()

            # 6. æ—¶æ•ˆæ€§æ£€æŸ¥
            print("\nâ° æ£€æŸ¥æ•°æ®æ—¶æ•ˆæ€§...")
            self._check_data_freshness()

            # 7. å¼‚å¸¸æ£€æµ‹
            print("\nğŸš¨ æ‰§è¡Œå¼‚å¸¸æ£€æµ‹...")
            self._detect_anomalies()

            # 8. ç”Ÿæˆç»¼åˆè¯„ä¼°
            print("\nğŸ“‹ ç”Ÿæˆç»¼åˆè¯„ä¼°æŠ¥å‘Š...")
            self._generate_comprehensive_assessment()

            print("\nâœ… æ•°æ®æºéªŒè¯å®Œæˆ")
            return self.validation_results

        except Exception as e:
            error_msg = f"æ•°æ®æºéªŒè¯è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}"
            print(f"âŒ {error_msg}")
            self.validation_results["overall_status"] = "error"
            self.validation_results["error"] = error_msg
            return self.validation_results

    def _test_data_source_connectivity(self):
        """æµ‹è¯•æ•°æ®æºè¿æ¥æ€§"""
        connectivity_results = {}

        # æµ‹è¯• YFinance è¿æ¥
        print("  ğŸ“ˆ æµ‹è¯• YFinance API...")
        try:
            start_time = time.time()
            test_data = self.yfinance_fetcher.execute("AAPL", "1d")
            response_time = time.time() - start_time

            connectivity_results["yfinance"] = {
                "status": "success" if test_data.get("success") else "failed",
                "response_time": response_time,
                "data_points": len(test_data.get("data", [])),
                "error": test_data.get("error"),
            }

            if response_time > 10:
                self.validation_results["issues_found"].append(
                    f"YFinanceå“åº”æ—¶é—´è¿‡é•¿: {response_time:.2f}ç§’"
                )

        except Exception as e:
            connectivity_results["yfinance"] = {"status": "error", "error": str(e)}
            self.validation_results["issues_found"].append(
                f"YFinanceè¿æ¥å¤±è´¥: {str(e)}"
            )

        # æµ‹è¯•åŸºæœ¬é¢æ•°æ®
        print("  ğŸ’° æµ‹è¯•åŸºæœ¬é¢æ•°æ®æº...")
        try:
            start_time = time.time()
            test_data = self.fundamental_fetcher.execute("AAPL")
            response_time = time.time() - start_time

            connectivity_results["fundamental"] = {
                "status": "success" if test_data.get("success") else "failed",
                "response_time": response_time,
                "metrics_count": len(
                    test_data.get("data", {}).get("financial_ratios", {})
                ),
                "error": test_data.get("error"),
            }

        except Exception as e:
            connectivity_results["fundamental"] = {"status": "error", "error": str(e)}
            self.validation_results["issues_found"].append(
                f"åŸºæœ¬é¢æ•°æ®è¿æ¥å¤±è´¥: {str(e)}"
            )

        # æµ‹è¯•æ–°é—»æ•°æ®
        print("  ğŸ“° æµ‹è¯•æ–°é—»æ•°æ®æº...")
        try:
            start_time = time.time()
            test_data = self.news_fetcher.execute("AAPL")
            response_time = time.time() - start_time

            connectivity_results["news"] = {
                "status": "success" if test_data.get("success") else "failed",
                "response_time": response_time,
                "articles_count": len(test_data.get("data", [])),
                "error": test_data.get("error"),
            }

        except Exception as e:
            connectivity_results["news"] = {"status": "error", "error": str(e)}
            self.validation_results["issues_found"].append(
                f"æ–°é—»æ•°æ®è¿æ¥å¤±è´¥: {str(e)}"
            )

        # æµ‹è¯•å®è§‚æ•°æ®
        print("  ğŸŒ æµ‹è¯•å®è§‚ç»æµæ•°æ®æº...")
        try:
            start_time = time.time()
            test_data = self.macro_fetcher.execute()
            response_time = time.time() - start_time

            connectivity_results["macro"] = {
                "status": "success" if test_data.get("success") else "failed",
                "response_time": response_time,
                "indicators_count": len(test_data.get("data", {})),
                "error": test_data.get("error"),
            }

        except Exception as e:
            connectivity_results["macro"] = {"status": "error", "error": str(e)}
            self.validation_results["issues_found"].append(
                f"å®è§‚æ•°æ®è¿æ¥å¤±è´¥: {str(e)}"
            )

        self.validation_results["data_sources"]["connectivity"] = connectivity_results

        # è¿æ¥æ€§æ±‡æ€»
        successful_sources = sum(
            1 for r in connectivity_results.values() if r.get("status") == "success"
        )
        total_sources = len(connectivity_results)

        print(f"  âœ… æ•°æ®æºè¿æ¥æµ‹è¯•å®Œæˆ: {successful_sources}/{total_sources} æˆåŠŸè¿æ¥")

    def _validate_data_quality(self):
        """éªŒè¯æ•°æ®è´¨é‡"""
        quality_results = {}

        # æŠ½æ ·æµ‹è¯•æ•°æ®è´¨é‡
        sample_stocks = ["AAPL", "MSFT", "GOOGL"]

        for data_type in ["fundamental", "technical", "sentiment", "macro"]:
            print(f"    ğŸ” éªŒè¯ {data_type} æ•°æ®è´¨é‡...")

            quality_metrics = {
                "completeness": 0.0,
                "accuracy": 0.0,
                "consistency": 0.0,
                "timeliness": 0.0,
                "issues": [],
            }

            try:
                if data_type == "fundamental":
                    quality_metrics = self._validate_fundamental_quality(sample_stocks)
                elif data_type == "technical":
                    quality_metrics = self._validate_technical_quality(sample_stocks)
                elif data_type == "sentiment":
                    quality_metrics = self._validate_sentiment_quality(sample_stocks)
                elif data_type == "macro":
                    quality_metrics = self._validate_macro_quality()

                quality_results[data_type] = quality_metrics

                # æ£€æŸ¥è´¨é‡æ ‡å‡†
                if (
                    quality_metrics["completeness"]
                    < self.quality_standards["minimum_coverage_ratio"]
                ):
                    self.validation_results["issues_found"].append(
                        f"{data_type}æ•°æ®å®Œæ•´æ€§ä¸è¶³: {quality_metrics['completeness']:.2%}"
                    )

            except Exception as e:
                quality_metrics["error"] = str(e)
                quality_results[data_type] = quality_metrics
                self.validation_results["issues_found"].append(
                    f"{data_type}æ•°æ®è´¨é‡éªŒè¯å¤±è´¥: {str(e)}"
                )

        self.validation_results["data_sources"]["quality"] = quality_results

    def _validate_fundamental_quality(self, stocks: List[str]) -> Dict:
        """éªŒè¯åŸºæœ¬é¢æ•°æ®è´¨é‡"""
        total_metrics = 0
        valid_metrics = 0
        accuracy_scores = []

        for stock in stocks:
            try:
                # ç¡®ä¿ä½¿ç”¨å®Œæ•´çš„è‚¡ç¥¨ä»£ç 
                if len(stock) < 2:
                    continue

                result = self.fundamental_fetcher.execute(stock)
                if result.get("success"):
                    data = result.get("data", {})
                    ratios = data.get("financial_ratios", {})

                    # æ£€æŸ¥å…³é”®æŒ‡æ ‡
                    key_ratios = ["pe_ratio", "roe", "debt_to_equity", "current_ratio"]
                    for ratio in key_ratios:
                        total_metrics += 1
                        if ratio in ratios and ratios[ratio] is not None:
                            valid_metrics += 1
                            # åŸºæœ¬åˆç†æ€§æ£€æŸ¥
                            value = ratios[ratio]
                            if isinstance(value, (int, float)) and not np.isnan(value):
                                if ratio == "pe_ratio" and 0 < value < 1000:
                                    accuracy_scores.append(1.0)
                                elif ratio == "roe" and -100 < value < 100:
                                    accuracy_scores.append(1.0)
                                elif ratio == "debt_to_equity" and 0 <= value < 10:
                                    accuracy_scores.append(1.0)
                                elif ratio == "current_ratio" and 0 < value < 20:
                                    accuracy_scores.append(1.0)
                                else:
                                    accuracy_scores.append(0.7)  # å¯ç–‘ä½†ä¸ä¸€å®šé”™è¯¯
                            else:
                                accuracy_scores.append(0.0)
                        else:
                            accuracy_scores.append(0.0)

            except Exception as e:
                # è®°å½•å…·ä½“çš„é”™è¯¯ä¿¡æ¯ä»¥ä¾¿è°ƒè¯•
                print(f"    âš ï¸ è‚¡ç¥¨ {stock} åŸºæœ¬é¢æ•°æ®éªŒè¯å¤±è´¥: {str(e)[:100]}...")
                continue

        completeness = valid_metrics / total_metrics if total_metrics > 0 else 0
        accuracy = np.mean(accuracy_scores) if accuracy_scores else 0

        return {
            "completeness": completeness,
            "accuracy": accuracy,
            "consistency": 0.9,  # åŸºæœ¬é¢æ•°æ®é€šå¸¸ä¸€è‡´æ€§è¾ƒå¥½
            "timeliness": 0.8,  # åŸºæœ¬é¢æ•°æ®æ›´æ–°é¢‘ç‡è¾ƒä½
            "total_metrics": total_metrics,
            "valid_metrics": valid_metrics,
            "issues": [],
        }

    def _validate_technical_quality(self, stocks: List[str]) -> Dict:
        """éªŒè¯æŠ€æœ¯é¢æ•°æ®è´¨é‡"""
        total_points = 0
        valid_points = 0
        variance_scores = []

        for stock in stocks:
            try:
                result = self.yfinance_fetcher.execute(stock, "1mo")
                if result.get("success"):
                    data = result.get("data", [])

                    if len(data) >= self.quality_standards["minimum_data_points"]:
                        total_points += len(data)

                        # æ£€æŸ¥æ•°æ®å®Œæ•´æ€§
                        for point in data:
                            if all(
                                key in point and point[key] is not None
                                for key in ["open", "high", "low", "close", "volume"]
                            ):
                                valid_points += 1

                        # æ£€æŸ¥ä»·æ ¼æ•°æ®æ–¹å·®
                        if len(data) > 1:
                            prices = [p["close"] for p in data if p.get("close")]
                            if len(prices) > 1:
                                price_variance = np.var(prices) / np.mean(prices) ** 2
                                variance_scores.append(
                                    1.0
                                    if price_variance
                                    > self.quality_standards["minimum_data_variance"]
                                    else 0.5
                                )

            except Exception as e:
                continue

        completeness = valid_points / total_points if total_points > 0 else 0
        variance_quality = np.mean(variance_scores) if variance_scores else 0.5

        return {
            "completeness": completeness,
            "accuracy": 0.95,  # æŠ€æœ¯æ•°æ®é€šå¸¸è¾ƒå‡†ç¡®
            "consistency": 0.9,
            "timeliness": 0.95,  # æŠ€æœ¯æ•°æ®æ›´æ–°åŠæ—¶
            "variance_quality": variance_quality,
            "total_points": total_points,
            "valid_points": valid_points,
            "issues": [],
        }

    def _validate_sentiment_quality(self, stocks: List[str]) -> Dict:
        """éªŒè¯æƒ…æ„Ÿé¢æ•°æ®è´¨é‡"""
        article_counts = []
        relevance_scores = []
        sentiment_diversities = []

        for stock in stocks:
            try:
                result = self.news_fetcher.execute(stock)
                if result.get("success"):
                    data = result.get("data", [])
                    article_counts.append(len(data))

                    # æ£€æŸ¥ç›¸å…³æ€§å’Œæƒ…æ„Ÿå¤šæ ·æ€§
                    if len(data) > 0:
                        # æ¨¡æ‹Ÿç›¸å…³æ€§è¯„åˆ†ï¼ˆå®é™…åº”è¯¥ä»æ•°æ®ä¸­è·å–ï¼‰
                        relevance_scores.append(0.8)  # é»˜è®¤ç›¸å…³æ€§

                        # è®¡ç®—æƒ…æ„Ÿå¤šæ ·æ€§
                        if len(data) > 1:
                            # è¿™é‡Œåº”è¯¥ä»å®é™…æƒ…æ„Ÿåˆ†æç»“æœä¸­è®¡ç®—
                            sentiment_diversities.append(0.7)

            except Exception as e:
                continue

        avg_articles = np.mean(article_counts) if article_counts else 0
        avg_relevance = np.mean(relevance_scores) if relevance_scores else 0
        avg_diversity = np.mean(sentiment_diversities) if sentiment_diversities else 0

        # è¯„ä¼°å®Œæ•´æ€§ï¼ˆåŸºäºæ–‡ç« æ•°é‡ï¼‰
        completeness = min(1.0, avg_articles / 10)  # ç†æƒ³æƒ…å†µ10ç¯‡æ–‡ç« 

        return {
            "completeness": completeness,
            "accuracy": avg_relevance,
            "consistency": avg_diversity,
            "timeliness": 0.9,  # æ–°é—»æ•°æ®æ—¶æ•ˆæ€§è¾ƒå¥½
            "avg_articles": avg_articles,
            "avg_relevance": avg_relevance,
            "issues": [],
        }

    def _validate_macro_quality(self) -> Dict:
        """éªŒè¯å®è§‚é¢æ•°æ®è´¨é‡"""
        try:
            result = self.macro_fetcher.execute()
            if result.get("success"):
                data = result.get("data", {})

                # æ£€æŸ¥å…³é”®æŒ‡æ ‡è¦†ç›–
                key_categories = ["interest_rates", "inflation", "employment", "growth"]
                covered_categories = sum(1 for cat in key_categories if cat in data)
                category_coverage = covered_categories / len(key_categories)

                # æ£€æŸ¥æŒ‡æ ‡æ•°é‡
                total_indicators = sum(
                    len(cat_data)
                    for cat_data in data.values()
                    if isinstance(cat_data, dict)
                )

                # éªŒè¯æ•°å€¼åˆç†æ€§
                reasonable_values = 0
                total_values = 0

                for category, indicators in data.items():
                    if isinstance(indicators, dict):
                        for indicator, value in indicators.items():
                            if isinstance(value, (int, float)) and not np.isnan(value):
                                total_values += 1
                                # åŸºæœ¬åˆç†æ€§æ£€æŸ¥
                                if "rate" in indicator.lower():
                                    if -5 <= value <= 25:  # åˆç†çš„åˆ©ç‡èŒƒå›´
                                        reasonable_values += 1
                                elif "unemployment" in indicator.lower():
                                    if 0 <= value <= 30:  # åˆç†çš„å¤±ä¸šç‡èŒƒå›´
                                        reasonable_values += 1
                                elif (
                                    "cpi" in indicator.lower()
                                    or "inflation" in indicator.lower()
                                ):
                                    if -10 <= value <= 20:  # åˆç†çš„é€šèƒ€èŒƒå›´
                                        reasonable_values += 1
                                else:
                                    reasonable_values += 1  # å…¶ä»–æŒ‡æ ‡é»˜è®¤åˆç†

                accuracy = reasonable_values / total_values if total_values > 0 else 0

                return {
                    "completeness": category_coverage,
                    "accuracy": accuracy,
                    "consistency": 0.85,  # å®è§‚æ•°æ®ä¸€è‡´æ€§è¾ƒå¥½
                    "timeliness": 0.75,  # å®è§‚æ•°æ®æ›´æ–°é¢‘ç‡è¾ƒä½
                    "total_indicators": total_indicators,
                    "category_coverage": category_coverage,
                    "issues": [],
                }
            else:
                return {
                    "completeness": 0.0,
                    "accuracy": 0.0,
                    "consistency": 0.0,
                    "timeliness": 0.0,
                    "issues": ["æ— æ³•è·å–å®è§‚æ•°æ®"],
                }

        except Exception as e:
            return {
                "completeness": 0.0,
                "accuracy": 0.0,
                "consistency": 0.0,
                "timeliness": 0.0,
                "issues": [f"å®è§‚æ•°æ®éªŒè¯é”™è¯¯: {str(e)}"],
            }

    def _check_data_consistency(self):
        """æ£€æŸ¥æ•°æ®ä¸€è‡´æ€§"""
        print("    ğŸ”„ æ£€æŸ¥è·¨æºæ•°æ®ä¸€è‡´æ€§...")

        consistency_results = {}

        # æ£€æŸ¥è‚¡ç¥¨ä»·æ ¼æ•°æ®ä¸€è‡´æ€§
        try:
            # è·å–åŒä¸€è‚¡ç¥¨çš„åŸºæœ¬é¢å’ŒæŠ€æœ¯é¢æ•°æ®
            stock = "AAPL"

            # åŸºæœ¬é¢æ•°æ®ä¸­çš„ä»·æ ¼ä¿¡æ¯
            fundamental_result = self.fundamental_fetcher.execute(stock)
            technical_result = self.yfinance_fetcher.execute(stock, "1d")

            if fundamental_result.get("success") and technical_result.get("success"):
                fund_data = fundamental_result.get("data", {})
                tech_data = technical_result.get("data", [])

                # æ¯”è¾ƒå¸‚å€¼ç­‰åŸºç¡€ä¿¡æ¯
                fund_market_cap = fund_data.get("basic_info", {}).get("market_cap")
                fund_price = fund_data.get("basic_info", {}).get("current_price")

                if tech_data and fund_price:
                    latest_tech_price = tech_data[-1].get("close")

                    if latest_tech_price and fund_price:
                        price_diff = abs(latest_tech_price - fund_price) / fund_price

                        consistency_results["price_consistency"] = {
                            "fundamental_price": fund_price,
                            "technical_price": latest_tech_price,
                            "difference_ratio": price_diff,
                            "consistent": price_diff
                            < self.quality_standards["consistency_tolerance"],
                        }

                        if price_diff > self.quality_standards["consistency_tolerance"]:
                            self.validation_results["issues_found"].append(
                                f"ä»·æ ¼æ•°æ®ä¸ä¸€è‡´: åŸºæœ¬é¢ {fund_price:.2f} vs æŠ€æœ¯é¢ {latest_tech_price:.2f}"
                            )

        except Exception as e:
            consistency_results["price_consistency"] = {"error": str(e)}

        # æ£€æŸ¥æ•°æ®æ—¶é—´æˆ³ä¸€è‡´æ€§
        try:
            current_time = datetime.now()
            timestamp_issues = []

            # æ£€æŸ¥å„æ•°æ®æºçš„æ—¶é—´æˆ³
            for source_name, fetcher in [
                ("technical", self.yfinance_fetcher),
                ("fundamental", self.fundamental_fetcher),
                ("news", self.news_fetcher),
            ]:
                if source_name == "technical":
                    result = fetcher.execute("AAPL", "1d")
                else:
                    result = fetcher.execute("AAPL")

                if result.get("success"):
                    timestamp_str = result.get("timestamp")
                    if timestamp_str:
                        try:
                            data_time = datetime.fromisoformat(
                                timestamp_str.replace("Z", "+00:00")
                            )
                            age_hours = (
                                current_time - data_time.replace(tzinfo=None)
                            ).total_seconds() / 3600

                            if (
                                age_hours
                                > 24 * self.quality_standards["maximum_age_days"]
                            ):
                                timestamp_issues.append(
                                    f"{source_name}æ•°æ®è¿‡æœŸ: {age_hours:.1f}å°æ—¶"
                                )

                        except Exception:
                            timestamp_issues.append(f"{source_name}æ—¶é—´æˆ³æ ¼å¼é”™è¯¯")

            consistency_results["timestamp_consistency"] = {
                "issues": timestamp_issues,
                "consistent": len(timestamp_issues) == 0,
            }

            if timestamp_issues:
                self.validation_results["issues_found"].extend(timestamp_issues)

        except Exception as e:
            consistency_results["timestamp_consistency"] = {"error": str(e)}

        self.validation_results["data_sources"]["consistency"] = consistency_results

    def _test_performance(self):
        """æµ‹è¯•æ€§èƒ½å’Œå“åº”æ—¶é—´"""
        print("    âš¡ æµ‹è¯•ç³»ç»Ÿæ€§èƒ½...")

        performance_results = {}

        # æµ‹è¯•å¹¶å‘åŠ è½½æ€§èƒ½
        try:
            start_time = time.time()

            # æ¨¡æ‹Ÿå¹¶å‘æ•°æ®åŠ è½½
            test_stocks = ["AAPL", "MSFT", "GOOGL"]
            results = self.data_loader.load_comprehensive_data(
                symbols=test_stocks, objective="balanced"
            )

            load_time = time.time() - start_time

            performance_results["comprehensive_load"] = {
                "load_time": load_time,
                "stocks_count": len(test_stocks),
                "time_per_stock": load_time / len(test_stocks),
                "success": results.get("success", False),
            }

            if load_time > 60:  # è¶…è¿‡1åˆ†é’Ÿ
                self.validation_results["issues_found"].append(
                    f"æ•°æ®åŠ è½½æ€§èƒ½è¾ƒæ…¢: {load_time:.2f}ç§’ for {len(test_stocks)}åªè‚¡ç¥¨"
                )

        except Exception as e:
            performance_results["comprehensive_load"] = {"error": str(e)}

        # æµ‹è¯•æ•°æ®é¢„å¤„ç†æ€§èƒ½
        try:
            start_time = time.time()

            # æ¨¡æ‹Ÿæ•°æ®é¢„å¤„ç†
            sample_data = {
                "AAPL": {"price": 150.0, "volume": 1000000},
                "MSFT": {"price": 300.0, "volume": 800000},
            }

            processed_data = self.preprocessor.create_unified_dataset(
                fundamental_data=sample_data,
                technical_data=sample_data,
                sentiment_data=sample_data,
                macro_data=sample_data,
            )

            process_time = time.time() - start_time

            performance_results["preprocessing"] = {
                "process_time": process_time,
                "data_points": len(sample_data),
                "success": processed_data is not None,
            }

        except Exception as e:
            performance_results["preprocessing"] = {"error": str(e)}

        self.validation_results["data_sources"]["performance"] = performance_results

    def _validate_data_completeness(self):
        """éªŒè¯æ•°æ®å®Œæ•´æ€§"""
        print("    âœ… éªŒè¯æ•°æ®å®Œæ•´æ€§...")

        completeness_results = {}

        # æ£€æŸ¥è‚¡ç¥¨è¦†ç›–ç‡
        try:
            test_stocks = self.test_stocks[:5]  # æµ‹è¯•å‰5åªè‚¡ç¥¨
            coverage_stats = {"total": 0, "successful": 0}

            for stock in test_stocks:
                coverage_stats["total"] += 1

                # æ£€æŸ¥æ˜¯å¦èƒ½è·å–åŸºç¡€æ•°æ®
                fund_result = self.fundamental_fetcher.execute(stock)
                tech_result = self.yfinance_fetcher.execute(stock, "1d")

                if fund_result.get("success") and tech_result.get("success"):
                    coverage_stats["successful"] += 1

            stock_coverage = coverage_stats["successful"] / coverage_stats["total"]

            completeness_results["stock_coverage"] = {
                "ratio": stock_coverage,
                "total_tested": coverage_stats["total"],
                "successful": coverage_stats["successful"],
                "meets_standard": stock_coverage
                >= self.quality_standards["minimum_coverage_ratio"],
            }

            if stock_coverage < self.quality_standards["minimum_coverage_ratio"]:
                self.validation_results["issues_found"].append(
                    f"è‚¡ç¥¨æ•°æ®è¦†ç›–ç‡ä¸è¶³: {stock_coverage:.2%}"
                )

        except Exception as e:
            completeness_results["stock_coverage"] = {"error": str(e)}

        # æ£€æŸ¥æ•°æ®å­—æ®µå®Œæ•´æ€§
        try:
            required_fields = {
                "fundamental": ["basic_info", "financial_ratios", "analysis"],
                "technical": ["open", "high", "low", "close", "volume"],
                "sentiment": ["overall_score", "relevance"],
                "macro": ["interest_rates", "inflation", "employment"],
            }

            field_completeness = {}

            # æµ‹è¯•åŸºæœ¬é¢å­—æ®µ
            fund_result = self.fundamental_fetcher.execute("AAPL")
            if fund_result.get("success"):
                fund_data = fund_result.get("data", {})
                fund_fields = sum(
                    1 for field in required_fields["fundamental"] if field in fund_data
                )
                field_completeness["fundamental"] = fund_fields / len(
                    required_fields["fundamental"]
                )

            # æµ‹è¯•æŠ€æœ¯é¢å­—æ®µ
            tech_result = self.yfinance_fetcher.execute("AAPL", "1d")
            if tech_result.get("success") and tech_result.get("data"):
                tech_data = (
                    tech_result.get("data", [])[0] if tech_result.get("data") else {}
                )
                tech_fields = sum(
                    1 for field in required_fields["technical"] if field in tech_data
                )
                field_completeness["technical"] = tech_fields / len(
                    required_fields["technical"]
                )

            completeness_results["field_completeness"] = field_completeness

        except Exception as e:
            completeness_results["field_completeness"] = {"error": str(e)}

        self.validation_results["data_sources"]["completeness"] = completeness_results

    def _check_data_freshness(self):
        """æ£€æŸ¥æ•°æ®æ—¶æ•ˆæ€§"""
        print("    â° æ£€æŸ¥æ•°æ®æ—¶æ•ˆæ€§...")

        freshness_results = {}
        current_time = datetime.now()

        # æ£€æŸ¥å„ç±»æ•°æ®çš„æ—¶æ•ˆæ€§
        data_sources = [
            ("technical", lambda: self.yfinance_fetcher.execute("AAPL", "1d")),
            ("fundamental", lambda: self.fundamental_fetcher.execute("AAPL")),
            ("news", lambda: self.news_fetcher.execute("AAPL")),
            ("macro", lambda: self.macro_fetcher.execute()),
        ]

        for source_name, data_func in data_sources:
            try:
                result = data_func()

                if result.get("success"):
                    timestamp_str = result.get("timestamp")
                    if timestamp_str:
                        try:
                            data_time = datetime.fromisoformat(
                                timestamp_str.replace("Z", "+00:00")
                            )
                            age_hours = (
                                current_time - data_time.replace(tzinfo=None)
                            ).total_seconds() / 3600
                            age_days = age_hours / 24

                            is_fresh = (
                                age_days <= self.quality_standards["maximum_age_days"]
                            )

                            freshness_results[source_name] = {
                                "timestamp": timestamp_str,
                                "age_hours": age_hours,
                                "age_days": age_days,
                                "is_fresh": is_fresh,
                                "max_age_days": self.quality_standards[
                                    "maximum_age_days"
                                ],
                            }

                            if not is_fresh:
                                self.validation_results["issues_found"].append(
                                    f"{source_name}æ•°æ®ä¸å¤Ÿæ–°é²œ: {age_days:.1f}å¤©å‰"
                                )

                        except Exception as e:
                            freshness_results[source_name] = {
                                "error": f"æ—¶é—´æˆ³è§£æé”™è¯¯: {str(e)}",
                                "is_fresh": False,
                            }
                    else:
                        freshness_results[source_name] = {
                            "error": "ç¼ºå°‘æ—¶é—´æˆ³",
                            "is_fresh": False,
                        }
                else:
                    freshness_results[source_name] = {
                        "error": "æ•°æ®è·å–å¤±è´¥",
                        "is_fresh": False,
                    }

            except Exception as e:
                freshness_results[source_name] = {"error": str(e), "is_fresh": False}

        self.validation_results["data_sources"]["freshness"] = freshness_results

    def _detect_anomalies(self):
        """æ£€æµ‹æ•°æ®å¼‚å¸¸"""
        print("    ğŸš¨ æ‰§è¡Œå¼‚å¸¸æ£€æµ‹...")

        anomaly_results = {}

        # ä»·æ ¼å¼‚å¸¸æ£€æµ‹
        try:
            price_anomalies = []

            for stock in ["AAPL", "MSFT", "GOOGL"]:
                result = self.yfinance_fetcher.execute(stock, "1mo")
                if result.get("success"):
                    data = result.get("data", [])
                    if len(data) > 10:
                        prices = [p["close"] for p in data if p.get("close")]

                        if len(prices) > 2:
                            # æ£€æµ‹ä»·æ ¼çªå˜
                            price_changes = [
                                abs(prices[i] - prices[i - 1]) / prices[i - 1]
                                for i in range(1, len(prices))
                            ]

                            max_change = max(price_changes) if price_changes else 0

                            if max_change > 0.2:  # å•æ—¥æ¶¨è·Œå¹…è¶…è¿‡20%
                                price_anomalies.append(
                                    {
                                        "stock": stock,
                                        "max_change": max_change,
                                        "description": f"{stock}å‘ç°ä»·æ ¼å¼‚å¸¸æ³¢åŠ¨: {max_change:.2%}",
                                    }
                                )

            anomaly_results["price_anomalies"] = price_anomalies

            if price_anomalies:
                for anomaly in price_anomalies:
                    self.validation_results["issues_found"].append(
                        anomaly["description"]
                    )

        except Exception as e:
            anomaly_results["price_anomalies"] = {"error": str(e)}

        # æ•°æ®å€¼å¼‚å¸¸æ£€æµ‹
        try:
            value_anomalies = []

            # æ£€æµ‹åŸºæœ¬é¢æ•°æ®å¼‚å¸¸
            result = self.fundamental_fetcher.execute("AAPL")
            if result.get("success"):
                ratios = result.get("data", {}).get("financial_ratios", {})

                # æ£€æµ‹å¼‚å¸¸çš„è´¢åŠ¡æ¯”ç‡
                if "pe_ratio" in ratios:
                    pe = ratios["pe_ratio"]
                    if isinstance(pe, (int, float)) and (pe < 0 or pe > 500):
                        value_anomalies.append(f"å¼‚å¸¸P/Eæ¯”ç‡: {pe}")

                if "debt_to_equity" in ratios:
                    de = ratios["debt_to_equity"]
                    if isinstance(de, (int, float)) and de > 10:
                        value_anomalies.append(f"å¼‚å¸¸å€ºåŠ¡æƒç›Šæ¯”: {de}")

            anomaly_results["value_anomalies"] = value_anomalies

            if value_anomalies:
                self.validation_results["issues_found"].extend(value_anomalies)

        except Exception as e:
            anomaly_results["value_anomalies"] = {"error": str(e)}

        self.validation_results["data_sources"]["anomalies"] = anomaly_results

    def _generate_comprehensive_assessment(self):
        """ç”Ÿæˆç»¼åˆè¯„ä¼°æŠ¥å‘Š"""
        print("    ğŸ“‹ ç”Ÿæˆç»¼åˆè¯„ä¼°...")

        try:
            # è®¡ç®—æ•´ä½“è´¨é‡åˆ†æ•°
            quality_scores = []

            # è¿æ¥æ€§è¯„åˆ†
            connectivity = self.validation_results["data_sources"].get(
                "connectivity", {}
            )
            successful_connections = sum(
                1 for r in connectivity.values() if r.get("status") == "success"
            )
            total_connections = len(connectivity)
            connectivity_score = (
                successful_connections / total_connections
                if total_connections > 0
                else 0
            )
            quality_scores.append(connectivity_score)

            # æ•°æ®è´¨é‡è¯„åˆ†
            quality_data = self.validation_results["data_sources"].get("quality", {})
            quality_score = 0
            quality_count = 0

            for data_type, metrics in quality_data.items():
                if isinstance(metrics, dict) and "completeness" in metrics:
                    quality_score += metrics["completeness"]
                    quality_count += 1

            if quality_count > 0:
                avg_quality = quality_score / quality_count
                quality_scores.append(avg_quality)

            # ä¸€è‡´æ€§è¯„åˆ†
            consistency = self.validation_results["data_sources"].get("consistency", {})
            consistency_score = 0.8  # é»˜è®¤åˆ†æ•°
            if "price_consistency" in consistency:
                price_consistent = consistency["price_consistency"].get(
                    "consistent", False
                )
                consistency_score = 0.9 if price_consistent else 0.6
            quality_scores.append(consistency_score)

            # æ—¶æ•ˆæ€§è¯„åˆ†
            freshness = self.validation_results["data_sources"].get("freshness", {})
            fresh_sources = sum(
                1 for f in freshness.values() if f.get("is_fresh", False)
            )
            total_sources = len(freshness)
            freshness_score = fresh_sources / total_sources if total_sources > 0 else 0
            quality_scores.append(freshness_score)

            # è®¡ç®—åŠ æƒå¹³å‡åˆ†
            weights = [0.3, 0.3, 0.2, 0.2]  # è¿æ¥æ€§ã€è´¨é‡ã€ä¸€è‡´æ€§ã€æ—¶æ•ˆæ€§
            overall_score = sum(
                score * weight for score, weight in zip(quality_scores, weights)
            )

            # ç¡®å®šæ•´ä½“çŠ¶æ€
            if overall_score >= 0.9:
                status = "excellent"
                status_desc = "ä¼˜ç§€"
            elif overall_score >= 0.8:
                status = "good"
                status_desc = "è‰¯å¥½"
            elif overall_score >= 0.7:
                status = "acceptable"
                status_desc = "å¯æ¥å—"
            elif overall_score >= 0.6:
                status = "poor"
                status_desc = "è¾ƒå·®"
            else:
                status = "critical"
                status_desc = "ä¸¥é‡é—®é¢˜"

            # ç”Ÿæˆå»ºè®®
            recommendations = []

            if connectivity_score < 0.8:
                recommendations.append("æ”¹å–„æ•°æ®æºè¿æ¥ç¨³å®šæ€§")

            if len(quality_scores) > 1 and quality_scores[1] < 0.8:
                recommendations.append("æå‡æ•°æ®è´¨é‡æ ‡å‡†")

            if consistency_score < 0.8:
                recommendations.append("åŠ å¼ºæ•°æ®ä¸€è‡´æ€§æ£€æŸ¥")

            if freshness_score < 0.8:
                recommendations.append("å¢åŠ æ•°æ®æ›´æ–°é¢‘ç‡")

            if len(self.validation_results["issues_found"]) > 5:
                recommendations.append("è§£å†³è¯†åˆ«å‡ºçš„æ•°æ®é—®é¢˜")

            # æ›´æ–°éªŒè¯ç»“æœ
            self.validation_results.update(
                {
                    "overall_status": status,
                    "overall_score": overall_score,
                    "status_description": status_desc,
                    "quality_breakdown": {
                        "connectivity": connectivity_score,
                        "quality": quality_scores[1] if len(quality_scores) > 1 else 0,
                        "consistency": consistency_score,
                        "freshness": freshness_score,
                    },
                    "recommendations": recommendations,
                }
            )

        except Exception as e:
            self.validation_results["overall_status"] = "error"
            self.validation_results["assessment_error"] = str(e)

    def generate_monitoring_report(self) -> str:
        """ç”Ÿæˆç›‘æ§æŠ¥å‘Š"""
        report = []
        report.append("ğŸ“Š æ•°æ®æºè´¨é‡ç›‘æ§æŠ¥å‘Š")
        report.append("=" * 50)
        report.append(f"ğŸ“… ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        report.append(
            f"ğŸ¯ æ•´ä½“çŠ¶æ€: {self.validation_results.get('status_description', 'æœªçŸ¥')}"
        )
        report.append(
            f"ğŸ“ˆ æ•´ä½“è¯„åˆ†: {self.validation_results.get('overall_score', 0):.2%}"
        )

        # è´¨é‡åˆ†è§£
        breakdown = self.validation_results.get("quality_breakdown", {})
        if breakdown:
            report.append("\nğŸ“Š è´¨é‡åˆ†è§£:")
            report.append(f"  ğŸ”— è¿æ¥æ€§: {breakdown.get('connectivity', 0):.2%}")
            report.append(f"  ğŸ“ˆ æ•°æ®è´¨é‡: {breakdown.get('quality', 0):.2%}")
            report.append(f"  ğŸ”„ ä¸€è‡´æ€§: {breakdown.get('consistency', 0):.2%}")
            report.append(f"  â°æ—¶æ•ˆæ€§: {breakdown.get('freshness', 0):.2%}")

        # å‘ç°çš„é—®é¢˜
        issues = self.validation_results.get("issues_found", [])
        if issues:
            report.append(f"\nâš ï¸ å‘ç°çš„é—®é¢˜ ({len(issues)}ä¸ª):")
            for i, issue in enumerate(issues[:10], 1):  # æ˜¾ç¤ºå‰10ä¸ªé—®é¢˜
                report.append(f"  {i}. {issue}")
            if len(issues) > 10:
                report.append(f"  ... è¿˜æœ‰ {len(issues) - 10} ä¸ªé—®é¢˜")

        # æ”¹è¿›å»ºè®®
        recommendations = self.validation_results.get("recommendations", [])
        if recommendations:
            report.append(f"\nğŸ’¡ æ”¹è¿›å»ºè®®:")
            for i, rec in enumerate(recommendations, 1):
                report.append(f"  {i}. {rec}")

        # æ•°æ®æºçŠ¶æ€
        connectivity = self.validation_results.get("data_sources", {}).get(
            "connectivity", {}
        )
        if connectivity:
            report.append("\nğŸ“¡ æ•°æ®æºè¿æ¥çŠ¶æ€:")
            for source, status in connectivity.items():
                status_icon = "âœ…" if status.get("status") == "success" else "âŒ"
                response_time = status.get("response_time", 0)
                report.append(f"  {status_icon} {source}: {response_time:.2f}ç§’")

        return "\n".join(report)

    def save_validation_results(self, filepath: str = "data_validation_results.json"):
        """ä¿å­˜éªŒè¯ç»“æœåˆ°æ–‡ä»¶"""
        try:
            with open(filepath, "w", encoding="utf-8") as f:
                json.dump(self.validation_results, f, ensure_ascii=False, indent=2)
            print(f"âœ… éªŒè¯ç»“æœå·²ä¿å­˜åˆ°: {filepath}")
        except Exception as e:
            print(f"âŒ ä¿å­˜éªŒè¯ç»“æœå¤±è´¥: {str(e)}")


def main():
    """ä¸»å‡½æ•° - è¿è¡Œæ•°æ®æºéªŒè¯"""
    print("ğŸ” OpenManus æ•°æ®æºéªŒè¯ç³»ç»Ÿ")
    print("=" * 60)

    try:
        # åˆ›å»ºéªŒè¯å™¨
        validator = DataSourceValidator()

        # è¿è¡Œå…¨é¢éªŒè¯
        results = validator.run_comprehensive_validation()

        # ç”Ÿæˆå¹¶æ˜¾ç¤ºæŠ¥å‘Š
        print("\n" + "=" * 60)
        report = validator.generate_monitoring_report()
        print(report)

        # ä¿å­˜ç»“æœ
        validator.save_validation_results()

        # è¿”å›çŠ¶æ€ç 
        status = results.get("overall_status", "error")
        if status in ["excellent", "good"]:
            return 0
        elif status == "acceptable":
            return 1
        else:
            return 2

    except Exception as e:
        print(f"âŒ éªŒè¯è¿‡ç¨‹å‘ç”Ÿé”™è¯¯: {str(e)}")
        traceback.print_exc()
        return 3


if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code)
