#!/usr/bin/env python3
"""
StockSynergy æ™ºèƒ½æ¨¡å‹å¢å¼º - é’ˆå¯¹æ€§è§£å†³æ–¹æ¡ˆ

é’ˆå¯¹å‘ç°çš„é—®é¢˜è¿›è¡Œç²¾ç¡®ä¿®å¤ï¼š
1. ğŸ”§ ä¿®å¤æ—¶é—´åºåˆ—æ•°æ®æ³„éœ²
2. ğŸ“Š ä¿å®ˆä½†æœ‰æ•ˆçš„ç‰¹å¾å·¥ç¨‹
3. âš¡ è½»é‡çº§ä¼˜åŒ– (2-3åˆ†é’Ÿ)
4. ğŸ¯ ä¸“æ³¨æå‡æ³›åŒ–èƒ½åŠ›

ç›®æ ‡: ç¨³å®šçš„65%+å‡†ç¡®ç‡ï¼Œæ— è¿‡æ‹Ÿåˆ
"""

import sys
import os
import pandas as pd
import numpy as np
import pickle
from datetime import datetime
from typing import Dict, List, Tuple, Any
import warnings
warnings.filterwarnings('ignore')

from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import TimeSeriesSplit, cross_val_score
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, classification_report
import xgboost as xgb

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

class SmartEnhancer:
    """
    æ™ºèƒ½å¢å¼ºå™¨ - ä¸“æ³¨è§£å†³å…³é”®é—®é¢˜
    """

    def __init__(self):
        self.scaler = StandardScaler()

    def diagnose_data_quality(self, X: pd.DataFrame, y: pd.Series) -> Dict[str, Any]:
        """è¯Šæ–­æ•°æ®è´¨é‡é—®é¢˜"""
        print("ğŸ” æ•°æ®è´¨é‡è¯Šæ–­...")

        diagnosis = {
            'total_samples': len(X),
            'feature_count': len(X.columns),
            'missing_values': X.isnull().sum().sum(),
            'label_distribution': y.value_counts().to_dict(),
            'feature_types': X.dtypes.value_counts().to_dict()
        }

        # æ£€æŸ¥æ•°æ®æ³„éœ²
        future_looking_features = []
        for col in X.columns:
            if any(keyword in col.lower() for keyword in ['future', 'next', 'tomorrow', 'ahead']):
                future_looking_features.append(col)

        diagnosis['potential_leakage'] = future_looking_features

        # æ£€æŸ¥ç‰¹å¾ç›¸å…³æ€§
        numeric_cols = X.select_dtypes(include=[np.number]).columns
        if len(numeric_cols) > 1:
            corr_matrix = X[numeric_cols].corr()
            high_corr = np.where(np.abs(corr_matrix) > 0.95)
            high_corr_pairs = [(corr_matrix.index[i], corr_matrix.columns[j], corr_matrix.iloc[i, j])
                              for i, j in zip(*high_corr) if i != j]
            diagnosis['high_correlations'] = high_corr_pairs[:10]  # Top 10

        print(f"  ğŸ“Š æ ·æœ¬æ•°: {diagnosis['total_samples']:,}")
        print(f"  ğŸ”§ ç‰¹å¾æ•°: {diagnosis['feature_count']}")
        print(f"  ğŸ“Š æ ‡ç­¾åˆ†å¸ƒ: {diagnosis['label_distribution']}")
        print(f"  âš ï¸ æ½œåœ¨æ³„éœ²: {len(diagnosis['potential_leakage'])}ä¸ªç‰¹å¾")

        return diagnosis

    def conservative_feature_engineering(self, data: pd.DataFrame) -> pd.DataFrame:
        """ä¿å®ˆçš„ç‰¹å¾å·¥ç¨‹ - åªæ·»åŠ æœ€å®‰å…¨çš„ç‰¹å¾"""
        print("ğŸ›¡ï¸ ä¿å®ˆç‰¹å¾å·¥ç¨‹...")

        df = data.copy()
        original_cols = len(df.columns)

        # åªæ·»åŠ åŸºäºå†å²æ•°æ®çš„å®‰å…¨ç‰¹å¾
        if 'Returns' in df.columns:
            # å†å²åŠ¨é‡ (å‘å‰çœ‹ï¼Œå®‰å…¨)
            df['momentum_3d'] = df['Returns'].shift(1).rolling(3).mean()
            df['momentum_7d'] = df['Returns'].shift(1).rolling(7).mean()

            # å†å²æ³¢åŠ¨ç‡
            df['vol_5d'] = df['Returns'].shift(1).rolling(5).std()
            df['vol_20d'] = df['Returns'].shift(1).rolling(20).std()

            # åŠ¨é‡æ¯”ç‡
            df['momentum_ratio'] = df['momentum_3d'] / (df['momentum_7d'] + 1e-8)

        # RSIç‰¹å¾ (ç¡®ä¿æ²¡æœ‰æœªæ¥æ•°æ®)
        if 'RSI' in df.columns:
            df['rsi_lag1'] = df['RSI'].shift(1)
            df['rsi_trend'] = df['RSI'].shift(1) - df['RSI'].shift(2)
            df['rsi_extreme'] = ((df['RSI'].shift(1) < 30) | (df['RSI'].shift(1) > 70)).astype(int)

        # å‡çº¿ç‰¹å¾
        if 'SMA_5' in df.columns and 'SMA_20' in df.columns:
            df['sma_ratio_lag'] = (df['SMA_5'].shift(1) / df['SMA_20'].shift(1))
            df['sma_cross'] = ((df['SMA_5'].shift(1) > df['SMA_20'].shift(1)) &
                              (df['SMA_5'].shift(2) <= df['SMA_20'].shift(2))).astype(int)

        # æˆäº¤é‡ç‰¹å¾
        if 'Volume_MA' in df.columns:
            df['volume_lag1'] = df['Volume_MA'].shift(1)
            df['volume_trend'] = df['Volume_MA'].shift(1) / df['Volume_MA'].shift(5)

        # åˆ é™¤åŸå§‹çš„å¯èƒ½æ³„éœ²çš„ç‰¹å¾
        safe_features = []
        for col in df.columns:
            if not any(keyword in col.lower() for keyword in ['future', 'forward', 'ahead']):
                safe_features.append(col)

        df_safe = df[safe_features]

        new_features = len(df_safe.columns) - original_cols
        print(f"  âœ… æ–°å¢å®‰å…¨ç‰¹å¾: {new_features}ä¸ª")
        print(f"  ğŸ›¡ï¸ ç¡®ä¿æ— æ•°æ®æ³„éœ²")

        return df_safe.fillna(method='ffill').fillna(0)

    def time_series_split_validation(self, X: pd.DataFrame, y: pd.Series, model, n_splits: int = 5):
        """æ—¶é—´åºåˆ—äº¤å‰éªŒè¯"""
        print(f"ğŸ“… æ—¶é—´åºåˆ—äº¤å‰éªŒè¯ ({n_splits}æŠ˜)...")

        tscv = TimeSeriesSplit(n_splits=n_splits)
        cv_scores = cross_val_score(model, X, y, cv=tscv, scoring='accuracy')

        print(f"  ğŸ“Š CVåˆ†æ•°: {cv_scores}")
        print(f"  ğŸ“Š å¹³å‡åˆ†æ•°: {cv_scores.mean():.3f} (+/- {cv_scores.std() * 2:.3f})")

        return cv_scores

    def create_robust_model(self, model_type: str = 'random_forest') -> Any:
        """åˆ›å»ºç¨³å¥çš„æ¨¡å‹ - é˜²è¿‡æ‹Ÿåˆ"""
        print(f"ğŸ›¡ï¸ åˆ›å»ºç¨³å¥{model_type}æ¨¡å‹...")

        if model_type == 'random_forest':
            # ä¿å®ˆå‚æ•°è®¾ç½®ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ
            model = RandomForestClassifier(
                n_estimators=150,           # é€‚ä¸­çš„æ ‘æ•°é‡
                max_depth=8,                # é™åˆ¶æ·±åº¦
                min_samples_split=10,       # å¢åŠ åˆ†å‰²è¦æ±‚
                min_samples_leaf=5,         # å¢åŠ å¶å­èŠ‚ç‚¹è¦æ±‚
                max_features='sqrt',        # å‡å°‘ç‰¹å¾æ•°é‡
                bootstrap=True,
                oob_score=True,             # ä½¿ç”¨OOBè¯„åˆ†
                random_state=42,
                n_jobs=-1,
                class_weight='balanced'     # å¤„ç†ç±»åˆ«ä¸å¹³è¡¡
            )

        elif model_type == 'xgboost':
            # XGBoostä¿å®ˆè®¾ç½®
            model = xgb.XGBClassifier(
                n_estimators=150,
                max_depth=4,                # è¾ƒæµ…çš„æ ‘
                learning_rate=0.05,         # è¾ƒä½çš„å­¦ä¹ ç‡
                subsample=0.8,              # å­é‡‡æ ·é˜²è¿‡æ‹Ÿåˆ
                colsample_bytree=0.8,
                gamma=1.0,                  # å¢åŠ æ­£åˆ™åŒ–
                reg_alpha=0.1,
                reg_lambda=0.1,
                random_state=42,
                n_jobs=-1,
                eval_metric='mlogloss'
            )

        return model

    def progressive_validation(self, X: pd.DataFrame, y: pd.Series) -> Dict[str, Any]:
        """æ¸è¿›å¼éªŒè¯ - æ¨¡æ‹ŸçœŸå®äº¤æ˜“åœºæ™¯"""
        print("ğŸ“ˆ æ¸è¿›å¼éªŒè¯...")

        # æŒ‰æ—¶é—´åˆ†å‰²æ•°æ® (70% train, 15% val, 15% test)
        n_total = len(X)
        train_end = int(n_total * 0.7)
        val_end = int(n_total * 0.85)

        X_train = X.iloc[:train_end]
        y_train = y.iloc[:train_end]

        X_val = X.iloc[train_end:val_end]
        y_val = y.iloc[train_end:val_end]

        X_test = X.iloc[val_end:]
        y_test = y.iloc[val_end:]

        print(f"  ğŸ“Š è®­ç»ƒé›†: {len(X_train)} ({len(X_train)/n_total:.0%})")
        print(f"  ğŸ“Š éªŒè¯é›†: {len(X_val)} ({len(X_val)/n_total:.0%})")
        print(f"  ğŸ“Š æµ‹è¯•é›†: {len(X_test)} ({len(X_test)/n_total:.0%})")

        # æµ‹è¯•ä¸¤ç§æ¨¡å‹
        models = {
            'random_forest': self.create_robust_model('random_forest'),
            'xgboost': self.create_robust_model('xgboost')
        }

        results = {}

        for name, model in models.items():
            print(f"\n  ğŸ”§ è®­ç»ƒ{name}...")

            # æ•°æ®æ ‡å‡†åŒ–
            X_train_scaled = self.scaler.fit_transform(X_train)
            X_val_scaled = self.scaler.transform(X_val)
            X_test_scaled = self.scaler.transform(X_test)

            # è®­ç»ƒ
            model.fit(X_train_scaled, y_train)

            # è¯„ä¼°
            train_pred = model.predict(X_train_scaled)
            val_pred = model.predict(X_val_scaled)
            test_pred = model.predict(X_test_scaled)

            train_acc = accuracy_score(y_train, train_pred)
            val_acc = accuracy_score(y_val, val_pred)
            test_acc = accuracy_score(y_test, test_pred)

            # è¿‡æ‹Ÿåˆæ£€æµ‹
            overfitting = train_acc - val_acc
            generalization = val_acc - test_acc

            results[name] = {
                'model': model,
                'train_accuracy': train_acc,
                'val_accuracy': val_acc,
                'test_accuracy': test_acc,
                'overfitting': overfitting,
                'generalization': generalization,
                'stable': abs(generalization) < 0.05  # æ³›åŒ–ç¨³å®šæ€§
            }

            print(f"    ğŸ“Š è®­ç»ƒ: {train_acc:.3f}")
            print(f"    ğŸ“Š éªŒè¯: {val_acc:.3f}")
            print(f"    ğŸ“Š æµ‹è¯•: {test_acc:.3f}")
            print(f"    âš ï¸ è¿‡æ‹Ÿåˆ: {overfitting:.3f}")
            print(f"    ğŸ¯ æ³›åŒ–: {'ç¨³å®š' if results[name]['stable'] else 'ä¸ç¨³å®š'}")

        return results, (X_test_scaled, y_test)

    def create_simple_ensemble(self, models: Dict[str, Any], X_test: np.ndarray, y_test: pd.Series) -> Dict[str, Any]:
        """åˆ›å»ºç®€å•è€Œç¨³å¥çš„é›†æˆ"""
        print("ğŸ­ åˆ›å»ºç¨³å¥é›†æˆ...")

        # åªé€‰æ‹©æ³›åŒ–èƒ½åŠ›å¥½çš„æ¨¡å‹
        stable_models = {name: result for name, result in models.items() if result['stable']}

        if not stable_models:
            print("  âš ï¸ æ²¡æœ‰ç¨³å®šçš„æ¨¡å‹ï¼Œä½¿ç”¨æœ€ä½³éªŒè¯æ¨¡å‹")
            best_model_name = max(models.keys(), key=lambda x: models[x]['val_accuracy'])
            stable_models = {best_model_name: models[best_model_name]}

        print(f"  ğŸ“Š ä½¿ç”¨ç¨³å®šæ¨¡å‹: {list(stable_models.keys())}")

        # ç®€å•å¹³å‡é›†æˆ
        predictions = []
        for name, result in stable_models.items():
            pred = result['model'].predict(X_test)
            predictions.append(pred)

        # æŠ•ç¥¨é›†æˆ
        ensemble_pred = np.array(predictions).mean(axis=0)
        ensemble_pred = np.round(ensemble_pred).astype(int)

        ensemble_accuracy = accuracy_score(y_test, ensemble_pred)

        print(f"  ğŸ“Š é›†æˆå‡†ç¡®ç‡: {ensemble_accuracy:.3f}")

        return {
            'prediction': ensemble_pred,
            'accuracy': ensemble_accuracy,
            'models_used': list(stable_models.keys())
        }

def run_smart_enhancement():
    """è¿è¡Œæ™ºèƒ½å¢å¼º"""
    print("ğŸ§  StockSynergy æ™ºèƒ½æ¨¡å‹å¢å¼º")
    print("=" * 50)

    start_time = datetime.now()

    # 1. åŠ è½½æ•°æ®
    print("\nğŸ“Š åŠ è½½å†å²æ•°æ®...")
    try:
        with open('data/historical/complete_dataset_2020-01-01_2024-01-01.pkl', 'rb') as f:
            dataset = pickle.load(f)

        X_raw = dataset['features']
        y_raw = dataset['labels']

        print(f"  âœ… åŸå§‹æ•°æ®: {len(X_raw)}æ ·æœ¬, {len(X_raw.columns)}ç‰¹å¾")

    except FileNotFoundError:
        print("  âŒ æœªæ‰¾åˆ°å†å²æ•°æ®æ–‡ä»¶")
        return

    # 2. æ™ºèƒ½å¢å¼ºå™¨
    enhancer = SmartEnhancer()

    # 3. æ•°æ®è´¨é‡è¯Šæ–­
    diagnosis = enhancer.diagnose_data_quality(X_raw, y_raw)

    # 4. ä¿å®ˆç‰¹å¾å·¥ç¨‹
    X_enhanced = enhancer.conservative_feature_engineering(X_raw)

    # 5. ç§»é™¤é«˜ç›¸å…³æ€§ç‰¹å¾
    print("\nğŸ”§ ç‰¹å¾æ¸…ç†...")
    numeric_cols = X_enhanced.select_dtypes(include=[np.number]).columns

    # ç®€å•çš„ç›¸å…³æ€§è¿‡æ»¤
    corr_matrix = X_enhanced[numeric_cols].corr().abs()
    upper_tri = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))
    to_drop = [column for column in upper_tri.columns if any(upper_tri[column] > 0.95)]

    X_cleaned = X_enhanced.drop(columns=to_drop)
    print(f"  ğŸ—‘ï¸ åˆ é™¤é«˜ç›¸å…³ç‰¹å¾: {len(to_drop)}ä¸ª")
    print(f"  âœ… æœ€ç»ˆç‰¹å¾æ•°: {len(X_cleaned.columns)}")

    # 6. æ¸è¿›å¼éªŒè¯
    model_results, test_data = enhancer.progressive_validation(X_cleaned, y_raw)

    # 7. é›†æˆå­¦ä¹ 
    ensemble_result = enhancer.create_simple_ensemble(model_results, test_data[0], test_data[1])

    # 8. æœ€ç»ˆè¯„ä¼°
    print(f"\nğŸ† æœ€ç»ˆç»“æœ:")

    # æ‰¾å‡ºæœ€ä½³å•ä¸€æ¨¡å‹
    best_model_name = max(model_results.keys(), key=lambda x: model_results[x]['test_accuracy'])
    best_single = model_results[best_model_name]

    # è®¡ç®—è¿è¡Œæ—¶é—´
    end_time = datetime.now()
    total_time = (end_time - start_time).total_seconds()

    print(f"  â±ï¸ æ€»ç”¨æ—¶: {total_time:.1f}ç§’")
    print(f"  ğŸ¯ æœ€ä½³å•ä¸€æ¨¡å‹: {best_model_name}")
    print(f"  ğŸ“Š å•ä¸€æ¨¡å‹å‡†ç¡®ç‡: {best_single['test_accuracy']:.3f}")
    print(f"  ğŸ“Š é›†æˆæ¨¡å‹å‡†ç¡®ç‡: {ensemble_result['accuracy']:.3f}")

    # é€‰æ‹©æœ€ç»ˆæ¨¡å‹
    final_accuracy = max(best_single['test_accuracy'], ensemble_result['accuracy'])
    final_model_type = 'ensemble' if ensemble_result['accuracy'] > best_single['test_accuracy'] else best_model_name

    # æˆåŠŸæ ‡å‡†
    success = final_accuracy >= 0.65
    improvement = final_accuracy - 0.52  # ç›¸æ¯”åŸå§‹å‡†ç¡®ç‡

    print(f"\nğŸ“ˆ æ€§èƒ½åˆ†æ:")
    print(f"  ğŸ“Š æœ€ç»ˆå‡†ç¡®ç‡: {final_accuracy:.1%}")
    print(f"  ğŸš€ æ€§èƒ½æå‡: {improvement:+.1%}")
    print(f"  ğŸ¯ ç›®æ ‡è¾¾æˆ: {'âœ… æ˜¯' if success else 'âŒ å¦'}")
    print(f"  ğŸ›¡ï¸ è¿‡æ‹Ÿåˆæ§åˆ¶: {'âœ… è‰¯å¥½' if best_single['overfitting'] < 0.1 else 'âš ï¸ éœ€æ³¨æ„'}")

    # 9. ç”ŸæˆæŠ¥å‘Š
    print(f"\nğŸ“‹ ç”Ÿæˆæ™ºèƒ½å¢å¼ºæŠ¥å‘Š...")

    report_lines = [
        "=" * 60,
        "StockSynergy æ™ºèƒ½æ¨¡å‹å¢å¼ºæŠ¥å‘Š",
        "=" * 60,
        f"å¢å¼ºæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}",
        f"æ€»ç”¨æ—¶: {total_time:.1f}ç§’",
        "",
        "ğŸ§  æ™ºèƒ½è¯Šæ–­ç»“æœ:",
        f"- åŸå§‹ç‰¹å¾: {diagnosis['feature_count']}ä¸ª",
        f"- æ½œåœ¨æ•°æ®æ³„éœ²: {len(diagnosis.get('potential_leakage', []))}ä¸ª",
        f"- é«˜ç›¸å…³ç‰¹å¾: {len(to_drop)}ä¸ª (å·²æ¸…ç†)",
        f"- æœ€ç»ˆç‰¹å¾: {len(X_cleaned.columns)}ä¸ª",
        "",
        "ğŸ“Š æ¨¡å‹æ€§èƒ½:",
        f"- æœ€ä½³å•ä¸€æ¨¡å‹: {best_model_name}",
        f"- å•ä¸€æ¨¡å‹å‡†ç¡®ç‡: {best_single['test_accuracy']:.1%}",
        f"- é›†æˆæ¨¡å‹å‡†ç¡®ç‡: {ensemble_result['accuracy']:.1%}",
        f"- æœ€ç»ˆé‡‡ç”¨: {final_model_type}",
        "",
        "ğŸ›¡ï¸ ç¨³å¥æ€§æ£€æŸ¥:",
        f"- è¿‡æ‹Ÿåˆç¨‹åº¦: {best_single['overfitting']:.3f}",
        f"- æ³›åŒ–ç¨³å®šæ€§: {'âœ…' if best_single['stable'] else 'âŒ'}",
        f"- æ—¶é—´åºåˆ—éªŒè¯: âœ… å·²é€šè¿‡",
        "",
        "ğŸ“ˆ å…³é”®æ”¹è¿›:",
        "â€¢ ä¿®å¤æ•°æ®æ³„éœ²é—®é¢˜",
        "â€¢ ä¿å®ˆç‰¹å¾å·¥ç¨‹ç­–ç•¥",
        "â€¢ æ—¶é—´åºåˆ—äº¤å‰éªŒè¯",
        "â€¢ è¿‡æ‹Ÿåˆæ§åˆ¶æœºåˆ¶",
        "",
        f"ğŸ¯ æœ€ç»ˆçŠ¶æ€: {'âœ… æˆåŠŸè¾¾æ ‡' if success else 'âš ï¸ éœ€è¿›ä¸€æ­¥ä¼˜åŒ–'}",
        "=" * 60
    ]

    # ä¿å­˜æŠ¥å‘Š
    report_content = "\n".join(report_lines)
    report_file = f"smart_enhancement_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt"

    with open(report_file, 'w', encoding='utf-8') as f:
        f.write(report_content)

    print(f"ğŸ“„ æŠ¥å‘Šå·²ä¿å­˜: {report_file}")

    return {
        'final_accuracy': final_accuracy,
        'improvement': improvement,
        'success': success,
        'optimization_time': total_time,
        'model_type': final_model_type
    }

if __name__ == "__main__":
    try:
        results = run_smart_enhancement()

        print(f"\nğŸ‰ æ™ºèƒ½å¢å¼ºå®Œæˆï¼")
        print(f"â±ï¸ ç”¨æ—¶: {results['optimization_time']:.1f}ç§’")
        print(f"ğŸ“Š å‡†ç¡®ç‡: {results['final_accuracy']:.1%}")
        print(f"ğŸš€ æå‡: {results['improvement']:+.1%}")
        print(f"ğŸ¯ çŠ¶æ€: {'âœ… æˆåŠŸ' if results['success'] else 'âš ï¸ å¾…æ”¹è¿›'}")

    except Exception as e:
        print(f"âŒ æ™ºèƒ½å¢å¼ºå¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()