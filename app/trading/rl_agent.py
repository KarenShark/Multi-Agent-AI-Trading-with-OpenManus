"""
å¼ºåŒ–å­¦ä¹ æ™ºèƒ½ä½“

åŸºäºæ·±åº¦å¼ºåŒ–å­¦ä¹ çš„äº¤æ˜“æ™ºèƒ½ä½“ï¼ŒåŒ…æ‹¬ï¼š
- PPO (Proximal Policy Optimization) Agent
- SAC (Soft Actor-Critic) Agent
- Custom Trading Agent
- Multi-Agent System
"""

import sys
import os
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
from torch.distributions import Categorical
from typing import Dict, List, Tuple, Any, Optional, Union
from collections import deque
import gymnasium as gym
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))

from app.trading.trading_env import TradingEnvironment, create_trading_environment

class PolicyNetwork(nn.Module):
    """ç­–ç•¥ç½‘ç»œ"""

    def __init__(
        self,
        input_dim: int,
        action_dim: int,
        hidden_dims: List[int] = [256, 128, 64],
        activation: str = "relu"
    ):
        super(PolicyNetwork, self).__init__()

        self.activation_fn = getattr(torch.nn.functional, activation)

        # æ„å»ºç½‘ç»œå±‚
        layers = []
        prev_dim = input_dim

        for hidden_dim in hidden_dims:
            layers.extend([
                nn.Linear(prev_dim, hidden_dim),
                nn.LayerNorm(hidden_dim),  # ä½¿ç”¨LayerNormæ›¿ä»£BatchNormé¿å…batch_size=1çš„é—®é¢˜
                nn.Dropout(0.2)
            ])
            prev_dim = hidden_dim

        # è¾“å‡ºå±‚
        layers.append(nn.Linear(prev_dim, action_dim))

        self.network = nn.Sequential(*layers)

        # åˆå§‹åŒ–æƒé‡
        self._init_weights()

    def _init_weights(self):
        """åˆå§‹åŒ–ç½‘ç»œæƒé‡"""
        for module in self.modules():
            if isinstance(module, nn.Linear):
                nn.init.orthogonal_(module.weight, gain=0.01)
                nn.init.constant_(module.bias, 0)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """å‰å‘ä¼ æ’­"""
        for i, layer in enumerate(self.network):
            if isinstance(layer, (nn.Linear, nn.LayerNorm, nn.Dropout)):
                x = layer(x)
                # åœ¨Linearå±‚ååº”ç”¨æ¿€æ´»å‡½æ•°ï¼ˆé™¤äº†æœ€åä¸€å±‚ï¼‰
                if isinstance(layer, nn.Linear) and i < len(self.network) - 1:
                    x = self.activation_fn(x)

        # æœ€åä¸€å±‚ä½¿ç”¨softmaxè·å¾—åŠ¨ä½œæ¦‚ç‡
        return torch.softmax(x, dim=-1)


class ValueNetwork(nn.Module):
    """ä»·å€¼ç½‘ç»œ"""

    def __init__(
        self,
        input_dim: int,
        hidden_dims: List[int] = [256, 128, 64],
        activation: str = "relu"
    ):
        super(ValueNetwork, self).__init__()

        self.activation_fn = getattr(torch.nn.functional, activation)

        # æ„å»ºç½‘ç»œå±‚
        layers = []
        prev_dim = input_dim

        for hidden_dim in hidden_dims:
            layers.extend([
                nn.Linear(prev_dim, hidden_dim),
                nn.LayerNorm(hidden_dim),  # ä½¿ç”¨LayerNormæ›¿ä»£BatchNormé¿å…batch_size=1çš„é—®é¢˜
                nn.Dropout(0.2)
            ])
            prev_dim = hidden_dim

        # è¾“å‡ºå±‚ (å•ä¸€ä»·å€¼)
        layers.append(nn.Linear(prev_dim, 1))

        self.network = nn.Sequential(*layers)

        # åˆå§‹åŒ–æƒé‡
        self._init_weights()

    def _init_weights(self):
        """åˆå§‹åŒ–ç½‘ç»œæƒé‡"""
        for module in self.modules():
            if isinstance(module, nn.Linear):
                nn.init.orthogonal_(module.weight)
                nn.init.constant_(module.bias, 0)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """å‰å‘ä¼ æ’­"""
        for i, layer in enumerate(self.network):
            if isinstance(layer, (nn.Linear, nn.LayerNorm, nn.Dropout)):
                x = layer(x)
                # åœ¨Linearå±‚ååº”ç”¨æ¿€æ´»å‡½æ•°ï¼ˆé™¤äº†æœ€åä¸€å±‚ï¼‰
                if isinstance(layer, nn.Linear) and i < len(self.network) - 1:
                    x = self.activation_fn(x)

        return x


class PPOAgent:
    """
    PPO (Proximal Policy Optimization) æ™ºèƒ½ä½“

    é€‚ç”¨äºè¿ç»­çŠ¶æ€ç©ºé—´å’Œç¦»æ•£åŠ¨ä½œç©ºé—´çš„äº¤æ˜“ç¯å¢ƒ
    """

    def __init__(
        self,
        state_dim: int,
        action_dim: int,
        n_symbols: int = 1,
        lr_policy: float = 3e-4,
        lr_value: float = 3e-4,
        gamma: float = 0.99,
        eps_clip: float = 0.2,
        k_epochs: int = 4,
        entropy_coef: float = 0.01,
        value_coef: float = 0.5,
        device: str = "cpu"
    ):
        self.state_dim = state_dim
        self.action_dim = action_dim
        self.n_symbols = n_symbols
        self.gamma = gamma
        self.eps_clip = eps_clip
        self.k_epochs = k_epochs
        self.entropy_coef = entropy_coef
        self.value_coef = value_coef
        self.device = torch.device(device)

        # ç½‘ç»œ
        self.policy_net = PolicyNetwork(state_dim, action_dim).to(self.device)
        self.value_net = ValueNetwork(state_dim).to(self.device)

        # ä¼˜åŒ–å™¨
        self.policy_optimizer = optim.Adam(self.policy_net.parameters(), lr=lr_policy)
        self.value_optimizer = optim.Adam(self.value_net.parameters(), lr=lr_value)

        # ç»éªŒç¼“å†²åŒº
        self.memory = {
            'states': [],
            'actions': [],
            'rewards': [],
            'log_probs': [],
            'values': [],
            'dones': []
        }

        # è®­ç»ƒç»Ÿè®¡
        self.training_stats = {
            'episodes': 0,
            'policy_losses': [],
            'value_losses': [],
            'total_rewards': [],
            'episode_lengths': []
        }

        print(f"ğŸ§  PPOæ™ºèƒ½ä½“åˆå§‹åŒ–:")
        print(f"  ğŸ“Š çŠ¶æ€ç»´åº¦: {state_dim}")
        print(f"  ğŸ® åŠ¨ä½œç»´åº¦: {action_dim}")
        print(f"  ğŸ’¼ è‚¡ç¥¨æ•°é‡: {n_symbols}")
        print(f"  ğŸ–¥ï¸ è®¾å¤‡: {device}")

    def select_action(self, state: np.ndarray, training: bool = True) -> Tuple[np.ndarray, torch.Tensor, torch.Tensor]:
        """é€‰æ‹©åŠ¨ä½œ"""
        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)

        with torch.no_grad():
            # è·å–åŠ¨ä½œæ¦‚ç‡
            action_probs = self.policy_net(state_tensor)

            # è·å–çŠ¶æ€ä»·å€¼
            value = self.value_net(state_tensor)

        if training:
            # è®­ç»ƒæ¨¡å¼ï¼šä»æ¦‚ç‡åˆ†å¸ƒä¸­é‡‡æ ·
            if self.n_symbols == 1:
                # å•è‚¡ç¥¨æƒ…å†µ
                dist = Categorical(action_probs)
                action = dist.sample()
                log_prob = dist.log_prob(action)
                action_array = np.array([action.item()])
            else:
                # å¤šè‚¡ç¥¨æƒ…å†µï¼šä¸ºæ¯åªè‚¡ç¥¨ç‹¬ç«‹é‡‡æ ·
                actions = []
                log_probs = []

                # å‡è®¾åŠ¨ä½œæ¦‚ç‡æ˜¯(batch_size, n_symbols * action_per_symbol)
                actions_per_symbol = self.action_dim // self.n_symbols

                for i in range(self.n_symbols):
                    start_idx = i * actions_per_symbol
                    end_idx = (i + 1) * actions_per_symbol
                    symbol_probs = action_probs[0, start_idx:end_idx]

                    dist = Categorical(symbol_probs)
                    action = dist.sample()
                    log_prob = dist.log_prob(action)

                    actions.append(action.item())
                    log_probs.append(log_prob)

                action_array = np.array(actions)
                log_prob = torch.stack(log_probs).sum()
        else:
            # æ¨ç†æ¨¡å¼ï¼šé€‰æ‹©æœ€å¤§æ¦‚ç‡åŠ¨ä½œ
            if self.n_symbols == 1:
                action = torch.argmax(action_probs, dim=1)
                action_array = np.array([action.item()])
                log_prob = torch.log(torch.max(action_probs))
            else:
                actions = []
                actions_per_symbol = self.action_dim // self.n_symbols

                for i in range(self.n_symbols):
                    start_idx = i * actions_per_symbol
                    end_idx = (i + 1) * actions_per_symbol
                    symbol_probs = action_probs[0, start_idx:end_idx]
                    action = torch.argmax(symbol_probs)
                    actions.append(action.item())

                action_array = np.array(actions)
                log_prob = torch.log(torch.max(action_probs))

        return action_array, log_prob, value.squeeze()

    def store_transition(
        self,
        state: np.ndarray,
        action: np.ndarray,
        reward: float,
        log_prob: torch.Tensor,
        value: torch.Tensor,
        done: bool
    ):
        """å­˜å‚¨è½¬ç§»ç»éªŒ"""
        self.memory['states'].append(state)
        self.memory['actions'].append(action)
        self.memory['rewards'].append(reward)
        self.memory['log_probs'].append(log_prob)
        self.memory['values'].append(value)
        self.memory['dones'].append(done)

    def update(self) -> Dict[str, float]:
        """æ›´æ–°ç­–ç•¥å’Œä»·å€¼ç½‘ç»œ"""
        if len(self.memory['states']) == 0:
            return {'policy_loss': 0, 'value_loss': 0}

        # è½¬æ¢ä¸ºå¼ é‡
        states = torch.FloatTensor(np.array(self.memory['states'])).to(self.device)
        actions = torch.LongTensor(np.array(self.memory['actions'])).to(self.device)
        rewards = torch.FloatTensor(self.memory['rewards']).to(self.device)
        old_log_probs = torch.stack(self.memory['log_probs']).to(self.device)
        old_values = torch.stack(self.memory['values']).to(self.device)
        dones = torch.BoolTensor(self.memory['dones']).to(self.device)

        # è®¡ç®—ä¼˜åŠ¿å‡½æ•°
        advantages, returns = self._compute_advantages(rewards, old_values, dones)

        # æ ‡å‡†åŒ–ä¼˜åŠ¿
        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)

        # PPOæ›´æ–°
        policy_losses = []
        value_losses = []

        for epoch in range(self.k_epochs):
            # é‡æ–°è®¡ç®—åŠ¨ä½œæ¦‚ç‡
            action_probs = self.policy_net(states)
            new_values = self.value_net(states).squeeze()

            # è®¡ç®—æ–°çš„logæ¦‚ç‡
            if self.n_symbols == 1:
                dist = Categorical(action_probs)
                new_log_probs = dist.log_prob(actions.squeeze())
                entropy = dist.entropy().mean()
            else:
                new_log_probs_list = []
                entropy_list = []
                actions_per_symbol = self.action_dim // self.n_symbols

                for i in range(self.n_symbols):
                    start_idx = i * actions_per_symbol
                    end_idx = (i + 1) * actions_per_symbol
                    symbol_probs = action_probs[:, start_idx:end_idx]

                    dist = Categorical(symbol_probs)
                    symbol_actions = actions[:, i] if len(actions.shape) > 1 else actions
                    new_log_prob = dist.log_prob(symbol_actions)

                    new_log_probs_list.append(new_log_prob)
                    entropy_list.append(dist.entropy())

                new_log_probs = torch.stack(new_log_probs_list).sum(dim=0)
                entropy = torch.stack(entropy_list).mean()

            # è®¡ç®—æ¯”ç‡
            ratio = torch.exp(new_log_probs - old_log_probs)

            # PPOæŸå¤±
            surr1 = ratio * advantages
            surr2 = torch.clamp(ratio, 1 - self.eps_clip, 1 + self.eps_clip) * advantages
            policy_loss = -torch.min(surr1, surr2).mean() - self.entropy_coef * entropy

            # ä»·å€¼æŸå¤±
            value_loss = nn.MSELoss()(new_values, returns)

            # æ›´æ–°ç½‘ç»œ
            self.policy_optimizer.zero_grad()
            policy_loss.backward()
            torch.nn.utils.clip_grad_norm_(self.policy_net.parameters(), 0.5)
            self.policy_optimizer.step()

            self.value_optimizer.zero_grad()
            value_loss.backward()
            torch.nn.utils.clip_grad_norm_(self.value_net.parameters(), 0.5)
            self.value_optimizer.step()

            policy_losses.append(policy_loss.item())
            value_losses.append(value_loss.item())

        # æ¸…ç©ºç»éªŒç¼“å†²åŒº
        self._clear_memory()

        avg_policy_loss = np.mean(policy_losses)
        avg_value_loss = np.mean(value_losses)

        # è®°å½•ç»Ÿè®¡ä¿¡æ¯
        self.training_stats['policy_losses'].append(avg_policy_loss)
        self.training_stats['value_losses'].append(avg_value_loss)

        return {
            'policy_loss': avg_policy_loss,
            'value_loss': avg_value_loss
        }

    def _compute_advantages(
        self,
        rewards: torch.Tensor,
        values: torch.Tensor,
        dones: torch.Tensor
    ) -> Tuple[torch.Tensor, torch.Tensor]:
        """è®¡ç®—ä¼˜åŠ¿å‡½æ•° (GAE)"""
        advantages = torch.zeros_like(rewards)
        returns = torch.zeros_like(rewards)

        gae = 0
        next_value = 0  # å‡è®¾æœ€åçŠ¶æ€çš„ä»·å€¼ä¸º0

        for step in reversed(range(len(rewards))):
            if step == len(rewards) - 1:
                next_non_terminal = 1.0 - dones[step].float()
                next_value = 0
            else:
                next_non_terminal = 1.0 - dones[step].float()
                next_value = values[step + 1]

            delta = rewards[step] + self.gamma * next_value * next_non_terminal - values[step]
            gae = delta + self.gamma * 0.95 * next_non_terminal * gae  # lambda = 0.95
            advantages[step] = gae
            returns[step] = advantages[step] + values[step]

        return advantages, returns

    def _clear_memory(self):
        """æ¸…ç©ºç»éªŒç¼“å†²åŒº"""
        for key in self.memory:
            self.memory[key].clear()

    def train_on_environment(
        self,
        env: TradingEnvironment,
        num_episodes: int = 100,
        max_steps_per_episode: int = 1000,
        update_frequency: int = 10,
        verbose: bool = True
    ) -> Dict[str, List[float]]:
        """åœ¨äº¤æ˜“ç¯å¢ƒä¸­è®­ç»ƒæ™ºèƒ½ä½“"""
        print(f"ğŸ“ å¼€å§‹PPOè®­ç»ƒ ({num_episodes}è½®)...")

        episode_rewards = []
        episode_lengths = []

        for episode in range(num_episodes):
            state, info = env.reset()
            episode_reward = 0
            episode_length = 0

            for step in range(max_steps_per_episode):
                # é€‰æ‹©åŠ¨ä½œ
                action, log_prob, value = self.select_action(state, training=True)

                # æ‰§è¡ŒåŠ¨ä½œ
                next_state, reward, terminated, truncated, info = env.step(action)

                # å­˜å‚¨ç»éªŒ
                done = terminated or truncated
                self.store_transition(state, action, reward, log_prob, value, done)

                # æ›´æ–°çŠ¶æ€
                state = next_state
                episode_reward += reward
                episode_length += 1

                if done:
                    break

            episode_rewards.append(episode_reward)
            episode_lengths.append(episode_length)

            # æ›´æ–°ç½‘ç»œ
            if (episode + 1) % update_frequency == 0:
                losses = self.update()

                if verbose:
                    recent_rewards = episode_rewards[-update_frequency:]
                    avg_reward = np.mean(recent_rewards)
                    print(f"  Episode {episode + 1}: å¹³å‡å¥–åŠ±={avg_reward:.3f}, "
                          f"ç­–ç•¥æŸå¤±={losses['policy_loss']:.4f}, "
                          f"ä»·å€¼æŸå¤±={losses['value_loss']:.4f}")

            # è®°å½•ç»Ÿè®¡ä¿¡æ¯
            self.training_stats['episodes'] = episode + 1
            self.training_stats['total_rewards'] = episode_rewards
            self.training_stats['episode_lengths'] = episode_lengths

        print(f"âœ… PPOè®­ç»ƒå®Œæˆ")
        print(f"  ğŸ“Š å¹³å‡å¥–åŠ±: {np.mean(episode_rewards):.3f}")
        print(f"  ğŸ“Š æœ€ä½³å¥–åŠ±: {max(episode_rewards):.3f}")

        return {
            'episode_rewards': episode_rewards,
            'episode_lengths': episode_lengths,
            'policy_losses': self.training_stats['policy_losses'],
            'value_losses': self.training_stats['value_losses']
        }

    def evaluate(
        self,
        env: TradingEnvironment,
        num_episodes: int = 10,
        verbose: bool = True
    ) -> Dict[str, Any]:
        """è¯„ä¼°æ™ºèƒ½ä½“"""
        print(f"ğŸ“Š è¯„ä¼°PPOæ™ºèƒ½ä½“ ({num_episodes}è½®)...")

        episode_results = []

        for episode in range(num_episodes):
            state, info = env.reset()
            episode_reward = 0
            episode_steps = 0

            while True:
                # è´ªå©ªç­–ç•¥é€‰æ‹©åŠ¨ä½œ
                action, _, _ = self.select_action(state, training=False)

                # æ‰§è¡ŒåŠ¨ä½œ
                state, reward, terminated, truncated, info = env.step(action)
                episode_reward += reward
                episode_steps += 1

                if terminated or truncated:
                    break

            episode_results.append({
                'episode': episode,
                'reward': episode_reward,
                'steps': episode_steps,
                'final_portfolio_value': info.get('portfolio_value', 0),
                'total_return': info.get('total_return', 0)
            })

            if verbose:
                print(f"  Episode {episode + 1}: å¥–åŠ±={episode_reward:.2f}, "
                      f"æŠ•èµ„ç»„åˆ=${info.get('portfolio_value', 0):,.2f}, "
                      f"æ”¶ç›Šç‡={info.get('total_return', 0):.2%}")

        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        rewards = [r['reward'] for r in episode_results]
        returns = [r['total_return'] for r in episode_results]

        evaluation_results = {
            'avg_reward': np.mean(rewards),
            'std_reward': np.std(rewards),
            'avg_return': np.mean(returns),
            'std_return': np.std(returns),
            'win_rate': len([r for r in returns if r > 0]) / len(returns),
            'best_return': max(returns),
            'worst_return': min(returns),
            'episode_results': episode_results
        }

        print(f"  ğŸ“Š å¹³å‡å¥–åŠ±: {evaluation_results['avg_reward']:.3f}")
        print(f"  ğŸ“Š å¹³å‡æ”¶ç›Šç‡: {evaluation_results['avg_return']:.2%}")
        print(f"  ğŸ“Š èƒœç‡: {evaluation_results['win_rate']:.2%}")

        return evaluation_results

    def save_model(self, filepath: str):
        """ä¿å­˜æ¨¡å‹"""
        torch.save({
            'policy_net_state_dict': self.policy_net.state_dict(),
            'value_net_state_dict': self.value_net.state_dict(),
            'policy_optimizer_state_dict': self.policy_optimizer.state_dict(),
            'value_optimizer_state_dict': self.value_optimizer.state_dict(),
            'training_stats': self.training_stats,
            'config': {
                'state_dim': self.state_dim,
                'action_dim': self.action_dim,
                'n_symbols': self.n_symbols,
                'gamma': self.gamma,
                'eps_clip': self.eps_clip,
                'k_epochs': self.k_epochs,
                'entropy_coef': self.entropy_coef,
                'value_coef': self.value_coef
            }
        }, filepath)
        print(f"ğŸ’¾ PPOæ¨¡å‹å·²ä¿å­˜åˆ°: {filepath}")

    def load_model(self, filepath: str):
        """åŠ è½½æ¨¡å‹"""
        checkpoint = torch.load(filepath, map_location=self.device)

        self.policy_net.load_state_dict(checkpoint['policy_net_state_dict'])
        self.value_net.load_state_dict(checkpoint['value_net_state_dict'])
        self.policy_optimizer.load_state_dict(checkpoint['policy_optimizer_state_dict'])
        self.value_optimizer.load_state_dict(checkpoint['value_optimizer_state_dict'])
        self.training_stats = checkpoint['training_stats']

        print(f"ğŸ“‚ PPOæ¨¡å‹å·²ä» {filepath} åŠ è½½")


class TradingRLAgent:
    """
    äº¤æ˜“å¼ºåŒ–å­¦ä¹ æ™ºèƒ½ä½“ç®¡ç†å™¨

    ç»Ÿä¸€ç®¡ç†ä¸åŒç±»å‹çš„RLæ™ºèƒ½ä½“
    """

    def __init__(self):
        self.agents = {}
        self.training_history = {}

    def create_ppo_agent(
        self,
        name: str,
        state_dim: int,
        action_dim: int,
        n_symbols: int = 1,
        **kwargs
    ) -> PPOAgent:
        """åˆ›å»ºPPOæ™ºèƒ½ä½“"""
        agent = PPOAgent(
            state_dim=state_dim,
            action_dim=action_dim,
            n_symbols=n_symbols,
            **kwargs
        )

        self.agents[name] = agent
        print(f"âœ… åˆ›å»ºPPOæ™ºèƒ½ä½“: {name}")

        return agent

    def train_agent(
        self,
        agent_name: str,
        env: TradingEnvironment,
        **kwargs
    ) -> Dict[str, Any]:
        """è®­ç»ƒæŒ‡å®šæ™ºèƒ½ä½“"""
        if agent_name not in self.agents:
            raise ValueError(f"æ™ºèƒ½ä½“ {agent_name} ä¸å­˜åœ¨")

        agent = self.agents[agent_name]
        results = agent.train_on_environment(env, **kwargs)

        self.training_history[agent_name] = results

        return results

    def evaluate_agent(
        self,
        agent_name: str,
        env: TradingEnvironment,
        **kwargs
    ) -> Dict[str, Any]:
        """è¯„ä¼°æŒ‡å®šæ™ºèƒ½ä½“"""
        if agent_name not in self.agents:
            raise ValueError(f"æ™ºèƒ½ä½“ {agent_name} ä¸å­˜åœ¨")

        agent = self.agents[agent_name]
        return agent.evaluate(env, **kwargs)

    def compare_agents(
        self,
        env: TradingEnvironment,
        num_episodes: int = 10
    ) -> Dict[str, Dict[str, Any]]:
        """æ¯”è¾ƒæ‰€æœ‰æ™ºèƒ½ä½“çš„æ€§èƒ½"""
        print(f"ğŸ† æ¯”è¾ƒæ™ºèƒ½ä½“æ€§èƒ½...")

        comparison_results = {}

        for agent_name, agent in self.agents.items():
            print(f"\nè¯„ä¼° {agent_name}:")
            results = agent.evaluate(env, num_episodes, verbose=False)
            comparison_results[agent_name] = results

        # æ˜¾ç¤ºæ¯”è¾ƒç»“æœ
        print(f"\nğŸ“Š æ€§èƒ½æ¯”è¾ƒ:")
        print(f"{'æ™ºèƒ½ä½“':<15} {'å¹³å‡æ”¶ç›Šç‡':<12} {'èƒœç‡':<8} {'å¤æ™®æ¯”ç‡':<10}")
        print("-" * 50)

        for agent_name, results in comparison_results.items():
            avg_return = results['avg_return']
            win_rate = results['win_rate']

            # è®¡ç®—å¤æ™®æ¯”ç‡ï¼ˆç®€åŒ–ç‰ˆï¼‰
            returns = [r['total_return'] for r in results['episode_results']]
            if len(returns) > 1:
                sharpe = np.mean(returns) / np.std(returns) if np.std(returns) > 0 else 0
            else:
                sharpe = 0

            print(f"{agent_name:<15} {avg_return:<12.2%} {win_rate:<8.2%} {sharpe:<10.3f}")

        return comparison_results


# ä¾¿æ·å‡½æ•°
def create_ppo_agent(
    state_dim: int,
    action_dim: int,
    n_symbols: int = 1,
    **kwargs
) -> PPOAgent:
    """åˆ›å»ºPPOæ™ºèƒ½ä½“"""
    return PPOAgent(
        state_dim=state_dim,
        action_dim=action_dim,
        n_symbols=n_symbols,
        **kwargs
    )


def create_trading_rl_manager() -> TradingRLAgent:
    """åˆ›å»ºäº¤æ˜“RLæ™ºèƒ½ä½“ç®¡ç†å™¨"""
    return TradingRLAgent()


# æµ‹è¯•ä»£ç 
if __name__ == "__main__":
    print("ğŸ§  å¼ºåŒ–å­¦ä¹ æ™ºèƒ½ä½“æµ‹è¯•")
    print("=" * 50)

    # åˆ›å»ºäº¤æ˜“ç¯å¢ƒ
    env = create_trading_environment(["AAPL", "MSFT"])

    state_dim = env.observation_space.shape[0]
    action_dim = 3  # 0=å–å‡º, 1=æŒæœ‰, 2=ä¹°å…¥
    n_symbols = len(env.symbols)

    print(f"ç¯å¢ƒä¿¡æ¯:")
    print(f"  çŠ¶æ€ç»´åº¦: {state_dim}")
    print(f"  åŠ¨ä½œç»´åº¦: {action_dim}")
    print(f"  è‚¡ç¥¨æ•°é‡: {n_symbols}")

    # æµ‹è¯•PPOæ™ºèƒ½ä½“
    print("\n1. æµ‹è¯•PPOæ™ºèƒ½ä½“:")
    ppo_agent = create_ppo_agent(
        state_dim=state_dim,
        action_dim=action_dim * n_symbols,  # æ¯åªè‚¡ç¥¨éƒ½æœ‰3ç§åŠ¨ä½œ
        n_symbols=n_symbols
    )

    # çŸ­æœŸè®­ç»ƒæµ‹è¯•
    print("\n2. çŸ­æœŸè®­ç»ƒæµ‹è¯•:")
    training_results = ppo_agent.train_on_environment(
        env,
        num_episodes=10,
        update_frequency=5
    )

    # è¯„ä¼°æµ‹è¯•
    print("\n3. è¯„ä¼°æµ‹è¯•:")
    evaluation_results = ppo_agent.evaluate(env, num_episodes=3)

    # æµ‹è¯•RLç®¡ç†å™¨
    print("\n4. æµ‹è¯•RLç®¡ç†å™¨:")
    rl_manager = create_trading_rl_manager()

    # åˆ›å»ºå¤šä¸ªæ™ºèƒ½ä½“
    agent1 = rl_manager.create_ppo_agent(
        "PPO_v1", state_dim, action_dim * n_symbols, n_symbols
    )

    print("\nâœ… å¼ºåŒ–å­¦ä¹ æ™ºèƒ½ä½“æµ‹è¯•å®Œæˆï¼")