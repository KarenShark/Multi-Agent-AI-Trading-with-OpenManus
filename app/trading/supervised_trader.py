"""
ç›‘ç£å­¦ä¹ äº¤æ˜“å‘˜

åŸºäºå†å²æ•°æ®è®­ç»ƒçš„ç›‘ç£å­¦ä¹ æ¨¡å‹ï¼ŒåŒ…æ‹¬ï¼š
- Random Forest Trader
- XGBoost Trader
- Neural Network Trader
- Ensemble Trader (é›†æˆå¤šæ¨¡å‹)
"""

import sys
import os
import numpy as np
import pandas as pd
from typing import Dict, List, Tuple, Any, Optional, Union
from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor
from sklearn.model_selection import train_test_split, GridSearchCV, TimeSeriesSplit
from sklearn.metrics import accuracy_score, classification_report, mean_squared_error
from sklearn.preprocessing import StandardScaler, LabelEncoder
import xgboost as xgb
import joblib
from datetime import datetime, timedelta
import warnings
warnings.filterwarnings('ignore')

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))

from app.trading.trading_env import TradingEnvironment, create_trading_environment
from app.data.loader import UnifiedDataLoader

class SupervisedTrader:
    """
    ç›‘ç£å­¦ä¹ äº¤æ˜“å‘˜åŸºç±»

    æä¾›åŸºç¡€çš„ç‰¹å¾å·¥ç¨‹ã€æ¨¡å‹è®­ç»ƒå’Œé¢„æµ‹åŠŸèƒ½
    """

    def __init__(
        self,
        model_type: str = "random_forest",
        prediction_horizon: int = 5,
        feature_engineering: bool = True,
        use_technical_indicators: bool = True,
        use_sentiment_features: bool = True,
        use_macro_features: bool = True
    ):
        self.model_type = model_type
        self.prediction_horizon = prediction_horizon
        self.feature_engineering = feature_engineering
        self.use_technical_indicators = use_technical_indicators
        self.use_sentiment_features = use_sentiment_features
        self.use_macro_features = use_macro_features

        # æ¨¡å‹ç»„ä»¶
        self.model = None
        self.scaler = StandardScaler()
        self.label_encoder = LabelEncoder()
        self.feature_columns = []

        # è®­ç»ƒå†å²
        self.training_history = {
            "train_scores": [],
            "val_scores": [],
            "feature_importance": {},
            "model_params": {}
        }

        # æ•°æ®åŠ è½½å™¨
        self.data_loader = UnifiedDataLoader()

        print(f"ğŸ¤– åˆå§‹åŒ–ç›‘ç£å­¦ä¹ äº¤æ˜“å‘˜:")
        print(f"  ğŸ“Š æ¨¡å‹ç±»å‹: {model_type}")
        print(f"  ğŸ”® é¢„æµ‹å‘¨æœŸ: {prediction_horizon}å¤©")
        print(f"  ğŸ› ï¸ ç‰¹å¾å·¥ç¨‹: {'å¯ç”¨' if feature_engineering else 'ç¦ç”¨'}")

    def prepare_training_data(
        self,
        symbols: List[str],
        start_date: str = "2022-01-01",
        end_date: str = "2024-01-01",
        objective: str = "balanced"
    ) -> Tuple[pd.DataFrame, pd.Series]:
        """å‡†å¤‡è®­ç»ƒæ•°æ®"""
        print("ğŸ“Š å‡†å¤‡è®­ç»ƒæ•°æ®...")

        try:
            # åŠ è½½ç»¼åˆæ•°æ®
            data_result = self.data_loader.load_comprehensive_data(
                symbols=symbols,
                objective=objective
            )

            if not data_result.get("success"):
                raise Exception("æ•°æ®åŠ è½½å¤±è´¥")

            # è·å–ç»Ÿä¸€æ•°æ®é›†
            feature_data = data_result.get("unified_dataset")
            if feature_data is None or feature_data.empty:
                raise Exception("ç»Ÿä¸€æ•°æ®é›†ä¸ºç©º")

            print(f"  âœ… åŠ è½½æ•°æ®æˆåŠŸ: {len(feature_data)}åªè‚¡ç¥¨ Ã— {len(feature_data.columns)}ç‰¹å¾")

            # ç”Ÿæˆæ¨¡æ‹Ÿå†å²ä»·æ ¼æ•°æ®ç”¨äºæ ‡ç­¾
            price_history = self._generate_price_history(symbols, start_date, end_date)

            # åˆ›å»ºç‰¹å¾å’Œæ ‡ç­¾
            X, y = self._create_features_and_labels(feature_data, price_history, symbols)

            print(f"  ğŸ“ˆ ç‰¹å¾çŸ©é˜µ: {X.shape}")
            print(f"  ğŸ¯ æ ‡ç­¾åˆ†å¸ƒ: {pd.Series(y).value_counts().to_dict()}")

            return X, y

        except Exception as e:
            print(f"  âŒ æ•°æ®å‡†å¤‡å¤±è´¥: {str(e)}")
            # ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®ä½œä¸ºfallback
            return self._generate_mock_training_data(symbols)

    def _generate_price_history(
        self,
        symbols: List[str],
        start_date: str,
        end_date: str
    ) -> Dict[str, np.ndarray]:
        """ç”Ÿæˆæ¨¡æ‹Ÿä»·æ ¼å†å²æ•°æ®"""

        start = datetime.strptime(start_date, "%Y-%m-%d")
        end = datetime.strptime(end_date, "%Y-%m-%d")
        days = (end - start).days

        price_history = {}

        for symbol in symbols:
            # åŸºç¡€ä»·æ ¼
            base_price = np.random.uniform(50, 200)

            # ç”Ÿæˆä»·æ ¼åºåˆ— (å‡ ä½•å¸ƒæœ—è¿åŠ¨)
            returns = np.random.normal(0.0005, 0.02, days)  # æ—¥æ”¶ç›Šç‡
            prices = [base_price]

            for ret in returns[1:]:
                prices.append(prices[-1] * (1 + ret))

            price_history[symbol] = np.array(prices)

        return price_history

    def _create_features_and_labels(
        self,
        feature_data: pd.DataFrame,
        price_history: Dict[str, np.ndarray],
        symbols: List[str]
    ) -> Tuple[pd.DataFrame, np.ndarray]:
        """åˆ›å»ºç‰¹å¾å’Œæ ‡ç­¾"""

        all_features = []
        all_labels = []

        for symbol in symbols:
            if symbol not in feature_data.index:
                continue

            # è·å–è¯¥è‚¡ç¥¨çš„ç‰¹å¾
            stock_features = feature_data.loc[symbol].to_dict()

            # è·å–ä»·æ ¼å†å²
            prices = price_history.get(symbol, [])
            if len(prices) < self.prediction_horizon + 1:
                continue

            # ä¸ºæ¯ä¸ªæ—¶é—´ç‚¹åˆ›å»ºæ ·æœ¬
            for i in range(len(prices) - self.prediction_horizon):
                # ç‰¹å¾ (å½“å‰ç‰¹å¾ + çŸ­æœŸä»·æ ¼å†å²)
                features = stock_features.copy()

                # æ·»åŠ ä»·æ ¼ç‰¹å¾
                current_price = prices[i]
                features['current_price'] = current_price

                if i > 0:
                    features['price_change_1d'] = (prices[i] - prices[i-1]) / prices[i-1]
                else:
                    features['price_change_1d'] = 0

                if i > 4:
                    features['price_change_5d'] = (prices[i] - prices[i-5]) / prices[i-5]
                    features['price_volatility_5d'] = np.std(prices[i-5:i]) / np.mean(prices[i-5:i])
                else:
                    features['price_change_5d'] = 0
                    features['price_volatility_5d'] = 0

                # æ ‡ç­¾ (æœªæ¥Nå¤©çš„æ”¶ç›Šç‡ç±»åˆ«)
                future_price = prices[i + self.prediction_horizon]
                future_return = (future_price - current_price) / current_price

                # å°†æ”¶ç›Šç‡åˆ†ä¸º3ç±»: 0=ä¸‹è·Œ, 1=æŒå¹³, 2=ä¸Šæ¶¨
                if future_return < -0.02:  # ä¸‹è·Œè¶…è¿‡2%
                    label = 0
                elif future_return > 0.02:  # ä¸Šæ¶¨è¶…è¿‡2%
                    label = 2
                else:  # æŒå¹³
                    label = 1

                all_features.append(features)
                all_labels.append(label)

        # è½¬æ¢ä¸ºDataFrame
        X = pd.DataFrame(all_features)

        # å¤„ç†ç¼ºå¤±å€¼
        X = X.fillna(X.mean())

        # ä¿å­˜ç‰¹å¾åˆ—å
        self.feature_columns = X.columns.tolist()

        return X, np.array(all_labels)

    def _generate_mock_training_data(self, symbols: List[str]) -> Tuple[pd.DataFrame, np.ndarray]:
        """ç”Ÿæˆæ¨¡æ‹Ÿè®­ç»ƒæ•°æ®"""
        print("  ğŸ”„ ç”Ÿæˆæ¨¡æ‹Ÿè®­ç»ƒæ•°æ®...")

        n_samples = 1000
        n_features = 20

        # ç”Ÿæˆç‰¹å¾
        X = pd.DataFrame(
            np.random.randn(n_samples, n_features),
            columns=[f'feature_{i}' for i in range(n_features)]
        )

        # ç”Ÿæˆæ ‡ç­¾ (åŸºäºç‰¹å¾çš„çº¿æ€§ç»„åˆ + å™ªå£°)
        weights = np.random.randn(n_features)
        scores = X.values @ weights

        # è½¬æ¢ä¸ºåˆ†ç±»æ ‡ç­¾
        y = np.where(scores < np.percentile(scores, 33), 0,
                    np.where(scores > np.percentile(scores, 67), 2, 1))

        self.feature_columns = X.columns.tolist()

        return X, y

    def train(
        self,
        X: pd.DataFrame,
        y: np.ndarray,
        test_size: float = 0.2,
        validate: bool = True,
        hyperparameter_tuning: bool = True
    ) -> Dict[str, Any]:
        """è®­ç»ƒæ¨¡å‹"""
        print(f"ğŸ¯ å¼€å§‹è®­ç»ƒ{self.model_type}æ¨¡å‹...")

        # æ•°æ®é¢„å¤„ç†
        X_scaled = self.scaler.fit_transform(X)

        # åˆ†å‰²æ•°æ® (æ—¶é—´åºåˆ—åˆ†å‰²)
        if validate:
            # ä½¿ç”¨æ—¶é—´åºåˆ—åˆ†å‰²ç¡®ä¿æœªæ¥æ•°æ®ä¸æ³„éœ²
            split_idx = int(len(X) * (1 - test_size))
            X_train, X_val = X_scaled[:split_idx], X_scaled[split_idx:]
            y_train, y_val = y[:split_idx], y[split_idx:]
        else:
            X_train, y_train = X_scaled, y
            X_val, y_val = None, None

        # åˆ›å»ºæ¨¡å‹
        self.model = self._create_model()

        # è¶…å‚æ•°è°ƒä¼˜
        if hyperparameter_tuning and validate:
            print("  ğŸ”§ æ‰§è¡Œè¶…å‚æ•°è°ƒä¼˜...")
            self.model = self._hyperparameter_tuning(X_train, y_train)

        # è®­ç»ƒæ¨¡å‹
        if self.model_type in ["random_forest", "xgboost"]:
            self.model.fit(X_train, y_train)
        else:
            # å¯¹äºå…¶ä»–æ¨¡å‹ç±»å‹ï¼Œå¯ä»¥æ·»åŠ early stoppingç­‰
            self.model.fit(X_train, y_train)

        # éªŒè¯
        train_results = {}
        if validate:
            train_score = self.model.score(X_train, y_train)
            val_score = self.model.score(X_val, y_val)

            train_results = {
                "train_accuracy": train_score,
                "val_accuracy": val_score,
                "overfitting": train_score - val_score
            }

            # è¯¦ç»†è¯„ä¼°
            y_pred = self.model.predict(X_val)
            train_results["classification_report"] = classification_report(
                y_val, y_pred, output_dict=True
            )

            print(f"  ğŸ“Š è®­ç»ƒå‡†ç¡®ç‡: {train_score:.3f}")
            print(f"  ğŸ“Š éªŒè¯å‡†ç¡®ç‡: {val_score:.3f}")
            print(f"  ğŸ“Š è¿‡æ‹Ÿåˆç¨‹åº¦: {train_score - val_score:.3f}")

        # ç‰¹å¾é‡è¦æ€§
        if hasattr(self.model, 'feature_importances_'):
            importance = dict(zip(self.feature_columns, self.model.feature_importances_))
            train_results["feature_importance"] = importance

            # æ˜¾ç¤ºTop 10é‡è¦ç‰¹å¾
            top_features = sorted(importance.items(), key=lambda x: x[1], reverse=True)[:10]
            print("  ğŸ”¥ Top 10é‡è¦ç‰¹å¾:")
            for feature, imp in top_features:
                print(f"    â€¢ {feature}: {imp:.3f}")

        # ä¿å­˜è®­ç»ƒå†å²
        self.training_history.update(train_results)

        print("  âœ… æ¨¡å‹è®­ç»ƒå®Œæˆ")
        return train_results

    def _create_model(self):
        """åˆ›å»ºæ¨¡å‹"""
        if self.model_type == "random_forest":
            return RandomForestClassifier(
                n_estimators=100,
                max_depth=10,
                min_samples_split=5,
                min_samples_leaf=2,
                random_state=42,
                n_jobs=-1
            )
        elif self.model_type == "xgboost":
            return xgb.XGBClassifier(
                n_estimators=100,
                max_depth=6,
                learning_rate=0.1,
                subsample=0.8,
                colsample_bytree=0.8,
                random_state=42,
                n_jobs=-1,
                eval_metric='mlogloss'
            )
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ¨¡å‹ç±»å‹: {self.model_type}")

    def _hyperparameter_tuning(self, X_train: np.ndarray, y_train: np.ndarray):
        """è¶…å‚æ•°è°ƒä¼˜"""

        if self.model_type == "random_forest":
            param_grid = {
                'n_estimators': [50, 100, 200],
                'max_depth': [5, 10, 15],
                'min_samples_split': [2, 5, 10],
                'min_samples_leaf': [1, 2, 4]
            }
            base_model = RandomForestClassifier(random_state=42, n_jobs=-1)

        elif self.model_type == "xgboost":
            param_grid = {
                'n_estimators': [50, 100, 200],
                'max_depth': [3, 6, 9],
                'learning_rate': [0.01, 0.1, 0.2],
                'subsample': [0.6, 0.8, 1.0]
            }
            base_model = xgb.XGBClassifier(random_state=42, n_jobs=-1, eval_metric='mlogloss')

        # æ—¶é—´åºåˆ—äº¤å‰éªŒè¯
        tscv = TimeSeriesSplit(n_splits=3)

        grid_search = GridSearchCV(
            base_model,
            param_grid,
            cv=tscv,
            scoring='accuracy',
            n_jobs=-1,
            verbose=0
        )

        grid_search.fit(X_train, y_train)

        print(f"    ğŸ¯ æœ€ä½³å‚æ•°: {grid_search.best_params_}")
        print(f"    ğŸ“Š æœ€ä½³å¾—åˆ†: {grid_search.best_score_:.3f}")

        return grid_search.best_estimator_

    def predict(self, features: Union[pd.DataFrame, np.ndarray]) -> Tuple[np.ndarray, np.ndarray]:
        """é¢„æµ‹"""
        if self.model is None:
            raise ValueError("æ¨¡å‹å°šæœªè®­ç»ƒ")

        # é¢„å¤„ç†ç‰¹å¾
        if isinstance(features, pd.DataFrame):
            # ç¡®ä¿ç‰¹å¾åˆ—é¡ºåºä¸€è‡´
            features = features[self.feature_columns]
            features_scaled = self.scaler.transform(features)
        else:
            features_scaled = self.scaler.transform(features)

        # é¢„æµ‹
        predictions = self.model.predict(features_scaled)

        # é¢„æµ‹æ¦‚ç‡
        if hasattr(self.model, 'predict_proba'):
            probabilities = self.model.predict_proba(features_scaled)
        else:
            probabilities = None

        return predictions, probabilities

    def predict_trading_action(self, features: Union[pd.DataFrame, np.ndarray]) -> np.ndarray:
        """é¢„æµ‹äº¤æ˜“åŠ¨ä½œ"""
        predictions, probabilities = self.predict(features)

        # è½¬æ¢é¢„æµ‹ç»“æœä¸ºäº¤æ˜“åŠ¨ä½œ
        # 0=ä¸‹è·Œé¢„æµ‹ -> 0=å–å‡º
        # 1=æŒå¹³é¢„æµ‹ -> 1=æŒæœ‰
        # 2=ä¸Šæ¶¨é¢„æµ‹ -> 2=ä¹°å…¥
        trading_actions = predictions.copy()

        return trading_actions

    def save_model(self, filepath: str):
        """ä¿å­˜æ¨¡å‹"""
        model_data = {
            'model': self.model,
            'scaler': self.scaler,
            'feature_columns': self.feature_columns,
            'model_type': self.model_type,
            'prediction_horizon': self.prediction_horizon,
            'training_history': self.training_history
        }

        joblib.dump(model_data, filepath)
        print(f"ğŸ’¾ æ¨¡å‹å·²ä¿å­˜åˆ°: {filepath}")

    def load_model(self, filepath: str):
        """åŠ è½½æ¨¡å‹"""
        model_data = joblib.load(filepath)

        self.model = model_data['model']
        self.scaler = model_data['scaler']
        self.feature_columns = model_data['feature_columns']
        self.model_type = model_data['model_type']
        self.prediction_horizon = model_data['prediction_horizon']
        self.training_history = model_data['training_history']

        print(f"ğŸ“‚ æ¨¡å‹å·²ä» {filepath} åŠ è½½")

    def evaluate_on_environment(
        self,
        env: TradingEnvironment,
        num_episodes: int = 10,
        verbose: bool = True
    ) -> Dict[str, Any]:
        """åœ¨äº¤æ˜“ç¯å¢ƒä¸­è¯„ä¼°æ¨¡å‹"""
        print(f"ğŸ® åœ¨äº¤æ˜“ç¯å¢ƒä¸­è¯„ä¼°æ¨¡å‹ ({num_episodes}è½®)...")

        episode_results = []

        for episode in range(num_episodes):
            obs, info = env.reset()
            episode_reward = 0
            episode_steps = 0

            while True:
                # å°†è§‚å¯Ÿè½¬æ¢ä¸ºç‰¹å¾æ ¼å¼
                features = self._obs_to_features(obs)

                # é¢„æµ‹äº¤æ˜“åŠ¨ä½œ
                try:
                    actions = self.predict_trading_action(features.reshape(1, -1))
                    action = actions[0] if isinstance(actions, np.ndarray) else actions

                    # ç¡®ä¿åŠ¨ä½œæ ¼å¼æ­£ç¡®
                    if isinstance(action, (int, np.integer)):
                        # å•è‚¡ç¥¨æƒ…å†µï¼Œæ‰©å±•ä¸ºå¤šè‚¡ç¥¨
                        action = [action] * len(env.symbols)
                    elif len(action) != len(env.symbols):
                        # å¤šè‚¡ç¥¨æƒ…å†µï¼Œç¡®ä¿é•¿åº¦åŒ¹é…
                        action = [action[0]] * len(env.symbols)

                except Exception as e:
                    # å¦‚æœé¢„æµ‹å¤±è´¥ï¼Œä½¿ç”¨éšæœºåŠ¨ä½œ
                    action = env.action_space.sample()

                # æ‰§è¡ŒåŠ¨ä½œ
                obs, reward, terminated, truncated, info = env.step(action)
                episode_reward += reward
                episode_steps += 1

                if terminated or truncated:
                    break

            episode_results.append({
                'episode': episode,
                'reward': episode_reward,
                'steps': episode_steps,
                'final_portfolio_value': info.get('portfolio_value', 0),
                'total_return': info.get('total_return', 0)
            })

            if verbose:
                print(f"  Episode {episode + 1}: å¥–åŠ±={episode_reward:.2f}, "
                      f"æŠ•èµ„ç»„åˆ=${info.get('portfolio_value', 0):,.2f}, "
                      f"æ”¶ç›Šç‡={info.get('total_return', 0):.2%}")

        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        rewards = [r['reward'] for r in episode_results]
        returns = [r['total_return'] for r in episode_results]

        evaluation_results = {
            'avg_reward': np.mean(rewards),
            'std_reward': np.std(rewards),
            'avg_return': np.mean(returns),
            'std_return': np.std(returns),
            'win_rate': len([r for r in returns if r > 0]) / len(returns),
            'best_return': max(returns),
            'worst_return': min(returns),
            'episode_results': episode_results
        }

        print(f"  ğŸ“Š å¹³å‡å¥–åŠ±: {evaluation_results['avg_reward']:.3f}")
        print(f"  ğŸ“Š å¹³å‡æ”¶ç›Šç‡: {evaluation_results['avg_return']:.2%}")
        print(f"  ğŸ“Š èƒœç‡: {evaluation_results['win_rate']:.2%}")

        return evaluation_results

    def _obs_to_features(self, observation: np.ndarray) -> np.ndarray:
        """å°†ç¯å¢ƒè§‚å¯Ÿè½¬æ¢ä¸ºæ¨¡å‹ç‰¹å¾"""
        # ç®€åŒ–ç‰ˆæœ¬ï¼šä½¿ç”¨å‰Nä¸ªè§‚å¯Ÿå€¼ä½œä¸ºç‰¹å¾
        # åœ¨å®é™…åº”ç”¨ä¸­ï¼Œè¿™é‡Œéœ€è¦æ›´å¤æ‚çš„ç‰¹å¾æ˜ å°„

        if len(observation) >= len(self.feature_columns):
            return observation[:len(self.feature_columns)]
        else:
            # å¦‚æœè§‚å¯Ÿç»´åº¦ä¸è¶³ï¼Œç”¨é›¶å¡«å……
            features = np.zeros(len(self.feature_columns))
            features[:len(observation)] = observation
            return features


class EnsembleTrader(SupervisedTrader):
    """
    é›†æˆäº¤æ˜“å‘˜

    ç»“åˆå¤šä¸ªæ¨¡å‹çš„é¢„æµ‹ç»“æœè¿›è¡Œäº¤æ˜“å†³ç­–
    """

    def __init__(self, model_types: List[str] = ["random_forest", "xgboost"], **kwargs):
        super().__init__(model_type="ensemble", **kwargs)
        self.model_types = model_types
        self.models = {}
        self.traders = {}

        print(f"ğŸ­ åˆå§‹åŒ–é›†æˆäº¤æ˜“å‘˜:")
        print(f"  ğŸ“Š é›†æˆæ¨¡å‹: {', '.join(model_types)}")

    def train(self, X: pd.DataFrame, y: np.ndarray, **kwargs) -> Dict[str, Any]:
        """è®­ç»ƒæ‰€æœ‰æ¨¡å‹"""
        print("ğŸ¯ è®­ç»ƒé›†æˆæ¨¡å‹...")

        ensemble_results = {}

        for model_type in self.model_types:
            print(f"\n  è®­ç»ƒ {model_type} æ¨¡å‹...")

            trader = SupervisedTrader(
                model_type=model_type,
                prediction_horizon=self.prediction_horizon
            )

            results = trader.train(X, y, **kwargs)

            self.traders[model_type] = trader
            ensemble_results[model_type] = results

        print("\nâœ… é›†æˆæ¨¡å‹è®­ç»ƒå®Œæˆ")
        return ensemble_results

    def predict_trading_action(self, features: Union[pd.DataFrame, np.ndarray]) -> np.ndarray:
        """é›†æˆé¢„æµ‹äº¤æ˜“åŠ¨ä½œ"""
        if not self.traders:
            raise ValueError("é›†æˆæ¨¡å‹å°šæœªè®­ç»ƒ")

        # æ”¶é›†æ‰€æœ‰æ¨¡å‹çš„é¢„æµ‹
        predictions = []
        for model_type, trader in self.traders.items():
            try:
                pred = trader.predict_trading_action(features)
                predictions.append(pred)
            except Exception as e:
                print(f"  âš ï¸ {model_type}é¢„æµ‹å¤±è´¥: {str(e)}")
                continue

        if not predictions:
            # å¦‚æœæ‰€æœ‰æ¨¡å‹éƒ½å¤±è´¥ï¼Œè¿”å›æŒæœ‰
            return np.array([1])

        # ç®€å•æŠ•ç¥¨æœºåˆ¶
        predictions_array = np.array(predictions)
        if len(predictions_array.shape) == 1:
            # å•ä¸ªé¢„æµ‹
            from scipy import stats
            final_prediction = stats.mode(predictions_array, keepdims=False)[0]
        else:
            # å¤šä¸ªé¢„æµ‹ï¼ŒæŒ‰åˆ—æŠ•ç¥¨
            final_prediction = []
            for col in range(predictions_array.shape[1]):
                from scipy import stats
                mode_result = stats.mode(predictions_array[:, col], keepdims=False)[0]
                final_prediction.append(mode_result)
            final_prediction = np.array(final_prediction)

        return final_prediction


# ä¾¿æ·å‡½æ•°
def create_supervised_trader(
    model_type: str = "random_forest",
    **kwargs
) -> SupervisedTrader:
    """åˆ›å»ºç›‘ç£å­¦ä¹ äº¤æ˜“å‘˜"""
    return SupervisedTrader(model_type=model_type, **kwargs)


def create_ensemble_trader(
    model_types: List[str] = ["random_forest", "xgboost"],
    **kwargs
) -> EnsembleTrader:
    """åˆ›å»ºé›†æˆäº¤æ˜“å‘˜"""
    return EnsembleTrader(model_types=model_types, **kwargs)


# æµ‹è¯•ä»£ç 
if __name__ == "__main__":
    print("ğŸ¤– ç›‘ç£å­¦ä¹ äº¤æ˜“å‘˜æµ‹è¯•")
    print("=" * 50)

    # æµ‹è¯•å•ä¸€æ¨¡å‹
    print("\n1. æµ‹è¯•Random Forestäº¤æ˜“å‘˜:")
    rf_trader = create_supervised_trader("random_forest")

    # å‡†å¤‡è®­ç»ƒæ•°æ®
    X, y = rf_trader.prepare_training_data(["AAPL", "MSFT"])

    # è®­ç»ƒæ¨¡å‹
    results = rf_trader.train(X, y)

    # æµ‹è¯•é›†æˆæ¨¡å‹
    print("\n2. æµ‹è¯•é›†æˆäº¤æ˜“å‘˜:")
    ensemble_trader = create_ensemble_trader(["random_forest", "xgboost"])
    ensemble_results = ensemble_trader.train(X, y)

    # åœ¨äº¤æ˜“ç¯å¢ƒä¸­æµ‹è¯•
    print("\n3. äº¤æ˜“ç¯å¢ƒæµ‹è¯•:")
    env = create_trading_environment(["AAPL", "MSFT"])
    evaluation = rf_trader.evaluate_on_environment(env, num_episodes=3)

    print("\nâœ… ç›‘ç£å­¦ä¹ äº¤æ˜“å‘˜æµ‹è¯•å®Œæˆï¼")