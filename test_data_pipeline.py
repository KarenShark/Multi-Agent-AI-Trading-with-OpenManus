#!/usr/bin/env python3
"""
æµ‹è¯•æ•°æ®é¢„å¤„ç†ç®¡é“åŠŸèƒ½
"""

import os
import tempfile
from datetime import datetime

import pandas as pd

from app.data.loader import UnifiedDataLoader
from app.data.preprocess import DataPreprocessor


def test_data_preprocessor():
    """æµ‹è¯•æ•°æ®é¢„å¤„ç†å™¨"""
    print("ğŸ” æµ‹è¯•æ•°æ®é¢„å¤„ç†å™¨...")

    preprocessor = DataPreprocessor()

    # æµ‹è¯•åŸºæœ¬é¢æ•°æ®é¢„å¤„ç†
    print("\nğŸ“Š æµ‹è¯•åŸºæœ¬é¢æ•°æ®é¢„å¤„ç†...")
    sample_fundamental = {
        "AAPL": {
            "raw_data": {
                "valuation": {
                    "trailing_pe": 25.5,
                    "price_to_book": 8.2,
                    "price_to_sales": 7.1,
                },
                "profitability": {
                    "return_on_equity": 0.175,
                    "profit_margin": 0.266,
                    "operating_margin": 0.298,
                },
                "liquidity": {"current_ratio": 0.98, "quick_ratio": 0.82},
                "leverage": {"debt_to_equity": 1.73, "interest_coverage": 29.1},
            }
        },
        "MSFT": {
            "raw_data": {
                "valuation": {
                    "trailing_pe": 35.2,
                    "price_to_book": 12.8,
                    "price_to_sales": 12.9,
                },
                "profitability": {
                    "return_on_equity": 0.458,
                    "profit_margin": 0.368,
                    "operating_margin": 0.421,
                },
            }
        },
    }

    processed_fundamental = preprocessor.preprocess_fundamental_data(sample_fundamental)

    if processed_fundamental and "AAPL" in processed_fundamental:
        print("âœ… åŸºæœ¬é¢æ•°æ®é¢„å¤„ç†æˆåŠŸ")
        aapl_data = processed_fundamental["AAPL"]
        print(f"  AAPL è´¨é‡è¯„åˆ†: {aapl_data.get('quality_score', 0):.2f}")

        feature_metrics = aapl_data.get("feature_metrics", {})
        if feature_metrics:
            print(f"  ä¼°å€¼è¯„åˆ†: {feature_metrics.get('valuation_score', 0):.2f}")
            print(
                f"  ç›ˆåˆ©èƒ½åŠ›è¯„åˆ†: {feature_metrics.get('profitability_score', 0):.2f}"
            )
            print(f"  è´¢åŠ¡å®åŠ›è¯„åˆ†: {feature_metrics.get('financial_strength', 0):.2f}")
    else:
        print("âŒ åŸºæœ¬é¢æ•°æ®é¢„å¤„ç†å¤±è´¥")
        return False

    # æµ‹è¯•æŠ€æœ¯é¢æ•°æ®é¢„å¤„ç†
    print("\nğŸ“ˆ æµ‹è¯•æŠ€æœ¯é¢æ•°æ®é¢„å¤„ç†...")
    sample_technical = {
        "AAPL": {
            "sma": list(range(150, 175)),  # 25ä¸ªæ•°æ®ç‚¹
            "rsi": [50 + i * 0.5 for i in range(25)],
            "macd": [i * 0.1 for i in range(25)],
        }
    }

    sample_price = {
        "AAPL": {
            "dates": [f"2024-01-{i+1:02d}" for i in range(25)],
            "values": list(range(170, 195)),
        }
    }

    processed_technical = preprocessor.preprocess_technical_data(
        sample_technical, sample_price
    )

    if processed_technical and "AAPL" in processed_technical:
        print("âœ… æŠ€æœ¯é¢æ•°æ®é¢„å¤„ç†æˆåŠŸ")
        aapl_tech = processed_technical["AAPL"]
        print(f"  AAPL è´¨é‡è¯„åˆ†: {aapl_tech.get('quality_score', 0):.2f}")
        print(f"  æ•°æ®ç‚¹æ•°é‡: {aapl_tech.get('data_points', 0)}")
        print(f"  ç‰¹å¾æ•°é‡: {aapl_tech.get('feature_count', 0)}")
    else:
        print("âŒ æŠ€æœ¯é¢æ•°æ®é¢„å¤„ç†å¤±è´¥")
        return False

    # æµ‹è¯•æƒ…æ„Ÿé¢æ•°æ®é¢„å¤„ç†
    print("\nğŸ’­ æµ‹è¯•æƒ…æ„Ÿé¢æ•°æ®é¢„å¤„ç†...")
    sample_sentiment = {
        "AAPL": {
            "scores": {"AAPL": 0.65},
            "metadata": {
                "article_count": 8,
                "relevance_score": 0.82,
                "time_weighted_sentiment": 0.71,
            },
        }
    }

    processed_sentiment = preprocessor.preprocess_sentiment_data(sample_sentiment)

    if processed_sentiment and "AAPL" in processed_sentiment:
        print("âœ… æƒ…æ„Ÿé¢æ•°æ®é¢„å¤„ç†æˆåŠŸ")
        aapl_sent = processed_sentiment["AAPL"]
        print(f"  AAPL è´¨é‡è¯„åˆ†: {aapl_sent.get('quality_score', 0):.2f}")

        sentiment_features = aapl_sent.get("sentiment_features", {})
        if sentiment_features:
            print(f"  æƒ…æ„Ÿå‡å€¼: {sentiment_features.get('sentiment_mean', 0):.3f}")
            print(f"  æ–°é—»æ•°é‡: {sentiment_features.get('news_volume', 0)}")
            print(f"  ç›¸å…³æ€§è¯„åˆ†: {sentiment_features.get('relevance_score', 0):.3f}")
    else:
        print("âŒ æƒ…æ„Ÿé¢æ•°æ®é¢„å¤„ç†å¤±è´¥")
        return False

    # æµ‹è¯•å®è§‚é¢æ•°æ®é¢„å¤„ç†
    print("\nğŸŒ æµ‹è¯•å®è§‚é¢æ•°æ®é¢„å¤„ç†...")
    sample_macro = {
        "interest_rates": {
            "fed_funds_rate": {"latest_value": 5.25},
            "10y_treasury": {"latest_value": 4.5},
        },
        "inflation": {"cpi": {"latest_value": 3.2}, "core_cpi": {"latest_value": 4.1}},
        "employment": {"unemployment_rate": {"latest_value": 3.7}},
    }

    processed_macro = preprocessor.preprocess_macro_data(sample_macro)

    if processed_macro and "macro_features" in processed_macro:
        print("âœ… å®è§‚é¢æ•°æ®é¢„å¤„ç†æˆåŠŸ")
        print(f"  è´¨é‡è¯„åˆ†: {processed_macro.get('quality_score', 0):.2f}")
        print(f"  æŒ‡æ ‡æ•°é‡: {processed_macro.get('indicators_count', 0)}")

        macro_features = processed_macro.get("macro_features", {})
        if macro_features:
            print(f"  åˆ©ç‡æ°´å¹³: {macro_features.get('interest_rate_level', 0):.2f}")
            print(f"  æ”¶ç›Šç‡æ›²çº¿æ–œç‡: {macro_features.get('yield_curve_slope', 0):.2f}")
            print(f"  é€šèƒ€æ°´å¹³: {macro_features.get('inflation_level', 0):.2f}")
    else:
        print("âŒ å®è§‚é¢æ•°æ®é¢„å¤„ç†å¤±è´¥")
        return False

    # æµ‹è¯•ç»Ÿä¸€æ•°æ®é›†åˆ›å»º
    print("\nğŸ”— æµ‹è¯•ç»Ÿä¸€æ•°æ®é›†åˆ›å»º...")
    symbols = ["AAPL", "MSFT"]

    unified_df = preprocessor.create_unified_dataset(
        processed_fundamental,
        processed_technical,
        processed_sentiment,
        processed_macro,
        symbols,
    )

    if not unified_df.empty:
        print("âœ… ç»Ÿä¸€æ•°æ®é›†åˆ›å»ºæˆåŠŸ")
        print(f"  æ•°æ®è¡Œæ•°: {len(unified_df)}")
        print(f"  ç‰¹å¾åˆ—æ•°: {len(unified_df.columns)}")
        print(f"  åŒ…å«è‚¡ç¥¨: {unified_df['symbol'].tolist()}")

        # æ˜¾ç¤ºéƒ¨åˆ†ç‰¹å¾
        feature_cols = [col for col in unified_df.columns if col != "symbol"][:5]
        print(f"  ç¤ºä¾‹ç‰¹å¾: {feature_cols}")
    else:
        print("âŒ ç»Ÿä¸€æ•°æ®é›†åˆ›å»ºå¤±è´¥")
        return False

    # æµ‹è¯•æ•°æ®è´¨é‡éªŒè¯
    print("\nğŸ” æµ‹è¯•æ•°æ®è´¨é‡éªŒè¯...")
    processed_data = {
        "fundamental": processed_fundamental,
        "technical": processed_technical,
        "sentiment": processed_sentiment,
        "macro": processed_macro,
    }

    quality_report = preprocessor.validate_data_quality(processed_data)

    if quality_report and "overall_score" in quality_report:
        print("âœ… æ•°æ®è´¨é‡éªŒè¯æˆåŠŸ")
        print(f"  æ•´ä½“è´¨é‡è¯„åˆ†: {quality_report['overall_score']:.2f}")

        category_scores = quality_report.get("category_scores", {})
        for category, score in category_scores.items():
            print(f"  {category} è´¨é‡: {score:.2f}")

        recommendations = quality_report.get("recommendations", [])
        if recommendations:
            print(f"  æ”¹è¿›å»ºè®®:")
            for rec in recommendations[:3]:  # æ˜¾ç¤ºå‰3ä¸ªå»ºè®®
                print(f"    â€¢ {rec}")
    else:
        print("âŒ æ•°æ®è´¨é‡éªŒè¯å¤±è´¥")
        return False

    return True


def test_unified_data_loader():
    """æµ‹è¯•ç»Ÿä¸€æ•°æ®åŠ è½½å™¨"""
    print("\nğŸ”„ æµ‹è¯•ç»Ÿä¸€æ•°æ®åŠ è½½å™¨...")

    # åˆ›å»ºä¸´æ—¶ç¼“å­˜ç›®å½•
    with tempfile.TemporaryDirectory() as temp_dir:
        config = {
            "enable_cache": True,
            "cache_directory": temp_dir,
            "cache_duration_hours": 1,
            "enable_preprocessing": True,
            "create_unified_dataset": True,
        }

        loader = UnifiedDataLoader(config)

        # æµ‹è¯•åŸºç¡€æ•°æ®åŠ è½½
        print("\nğŸ“Š æµ‹è¯•ç»¼åˆæ•°æ®åŠ è½½...")
        test_symbols = ["AAPL", "MSFT"]

        comprehensive_data = loader.load_comprehensive_data(
            symbols=test_symbols,
            objective="balanced",
            use_cache=False,  # é¦–æ¬¡åŠ è½½ä¸ä½¿ç”¨ç¼“å­˜
        )

        if "error" not in comprehensive_data:
            print("âœ… ç»¼åˆæ•°æ®åŠ è½½æˆåŠŸ")

            # æ£€æŸ¥æ•°æ®æº
            data_sources = comprehensive_data.get("metadata", {}).get(
                "data_sources", []
            )
            print(f"  æ•°æ®æº: {', '.join(data_sources)}")

            # æ£€æŸ¥åŸºæœ¬é¢æ•°æ®
            if "fundamental" in comprehensive_data:
                fund_data = comprehensive_data["fundamental"]
                print(f"  åŸºæœ¬é¢æ•°æ®: {len(fund_data)} åªè‚¡ç¥¨")

            # æ£€æŸ¥æŠ€æœ¯é¢æ•°æ®
            if "technical" in comprehensive_data:
                tech_data = comprehensive_data["technical"]
                print(f"  æŠ€æœ¯é¢æ•°æ®: {len(tech_data)} åªè‚¡ç¥¨")

            # æ£€æŸ¥æƒ…æ„Ÿé¢æ•°æ®
            if "sentiment" in comprehensive_data:
                sent_data = comprehensive_data["sentiment"]
                print(f"  æƒ…æ„Ÿé¢æ•°æ®: {len(sent_data)} åªè‚¡ç¥¨")

            # æ£€æŸ¥å®è§‚é¢æ•°æ®
            if "macro" in comprehensive_data:
                macro_data = comprehensive_data["macro"]
                print(f"  å®è§‚é¢æ•°æ®: å·²åŠ è½½")

            # æ£€æŸ¥åˆ†æç»“æœ
            if "analysis" in comprehensive_data:
                analysis = comprehensive_data["analysis"]
                analysis_types = list(analysis.keys())
                print(f"  æ™ºèƒ½åˆ†æ: {', '.join(analysis_types)}")

            # æ£€æŸ¥é¢„å¤„ç†ç»“æœ
            if "processed" in comprehensive_data:
                processed = comprehensive_data["processed"]
                if "quality_report" in processed:
                    quality = processed["quality_report"]
                    overall_score = quality.get("overall_score", 0)
                    print(f"  æ•°æ®è´¨é‡: {overall_score:.2f}")

            # æ£€æŸ¥ç»Ÿä¸€æ•°æ®é›†
            if "unified_dataset" in comprehensive_data:
                unified_df = comprehensive_data["unified_dataset"]
                if isinstance(unified_df, pd.DataFrame) and not unified_df.empty:
                    print(
                        f"  ç»Ÿä¸€æ•°æ®é›†: {len(unified_df)} è¡Œ, {len(unified_df.columns)} åˆ—"
                    )
                else:
                    print("  ç»Ÿä¸€æ•°æ®é›†: åˆ›å»ºå¤±è´¥æˆ–ä¸ºç©º")

        else:
            print(f"âŒ ç»¼åˆæ•°æ®åŠ è½½å¤±è´¥: {comprehensive_data['error']}")
            return False

        # æµ‹è¯•ç¼“å­˜åŠŸèƒ½
        print("\nğŸ“¦ æµ‹è¯•ç¼“å­˜åŠŸèƒ½...")

        # ç¬¬äºŒæ¬¡åŠ è½½åº”è¯¥ä½¿ç”¨ç¼“å­˜
        cached_data = loader.load_comprehensive_data(
            symbols=test_symbols, objective="balanced", use_cache=True
        )

        if "error" not in cached_data:
            print("âœ… ç¼“å­˜æ•°æ®åŠ è½½æˆåŠŸ")

            # æ£€æŸ¥ç¼“å­˜çŠ¶æ€
            cache_status = loader.get_cache_status()
            if cache_status.get("status") == "enabled":
                print(f"  ç¼“å­˜æ–‡ä»¶æ•°é‡: {cache_status.get('cached_files', 0)}")
        else:
            print(f"âŒ ç¼“å­˜æ•°æ®åŠ è½½å¤±è´¥: {cached_data['error']}")
            return False

        # æµ‹è¯•ç‰¹å®šæ•°æ®ç±»å‹åŠ è½½
        print("\nğŸ¯ æµ‹è¯•ç‰¹å®šæ•°æ®ç±»å‹åŠ è½½...")

        # æµ‹è¯•ä»…åŠ è½½åŸºæœ¬é¢æ•°æ®
        fundamental_only = loader.load_specific_data("fundamental", test_symbols)
        if "error" not in fundamental_only:
            print("âœ… åŸºæœ¬é¢æ•°æ®ä¸“é¡¹åŠ è½½æˆåŠŸ")
        else:
            print(f"âŒ åŸºæœ¬é¢æ•°æ®ä¸“é¡¹åŠ è½½å¤±è´¥: {fundamental_only['error']}")

        # æµ‹è¯•ä»…åŠ è½½å®è§‚æ•°æ®
        macro_only = loader.load_specific_data("macro")
        if "error" not in macro_only:
            print("âœ… å®è§‚æ•°æ®ä¸“é¡¹åŠ è½½æˆåŠŸ")
        else:
            print(f"âŒ å®è§‚æ•°æ®ä¸“é¡¹åŠ è½½å¤±è´¥: {macro_only['error']}")

        # æµ‹è¯•å®æ—¶å¿«ç…§
        print("\nâš¡ æµ‹è¯•å®æ—¶å¿«ç…§...")
        snapshot = loader.get_realtime_snapshot(["AAPL"])

        if "error" not in snapshot:
            print("âœ… å®æ—¶å¿«ç…§è·å–æˆåŠŸ")
            snapshot_data = snapshot.get("data", {})
            if "AAPL" in snapshot_data:
                aapl_snapshot = snapshot_data["AAPL"]
                latest_price = aapl_snapshot.get("latest_price")
                if latest_price:
                    print(f"  AAPL æœ€æ–°ä»·æ ¼: ${latest_price:.2f}")

                tech_signal = aapl_snapshot.get("technical_signal")
                if tech_signal:
                    action = tech_signal.get("action", "N/A")
                    confidence = tech_signal.get("confidence", 0)
                    print(f"  æŠ€æœ¯ä¿¡å·: {action} (ç½®ä¿¡åº¦: {confidence:.3f})")
        else:
            print(f"âŒ å®æ—¶å¿«ç…§è·å–å¤±è´¥: {snapshot['error']}")

        # æµ‹è¯•æ•°æ®è´¨é‡æŠ¥å‘Š
        print("\nğŸ“‹ æµ‹è¯•æ•°æ®è´¨é‡æŠ¥å‘Š...")
        quality_report = loader.get_data_quality_report(comprehensive_data)

        if "error" not in quality_report:
            print("âœ… æ•°æ®è´¨é‡æŠ¥å‘Šç”ŸæˆæˆåŠŸ")
            overall_assessment = quality_report.get("overall_assessment", "N/A")
            print(f"  æ•´ä½“è¯„ä¼°: {overall_assessment}")

            source_quality = quality_report.get("source_quality", {})
            for source, quality in source_quality.items():
                print(f"  {source} è´¨é‡: {quality:.2f}")

            recommendations = quality_report.get("recommendations", [])
            if recommendations:
                print(f"  ä¸»è¦å»ºè®®: {recommendations[0]}")
        else:
            print(f"âŒ æ•°æ®è´¨é‡æŠ¥å‘Šç”Ÿæˆå¤±è´¥: {quality_report['error']}")

        # æµ‹è¯•ç¼“å­˜ç®¡ç†
        print("\nğŸ—‚ï¸ æµ‹è¯•ç¼“å­˜ç®¡ç†...")

        # è·å–ç¼“å­˜çŠ¶æ€
        cache_status = loader.get_cache_status()
        if cache_status.get("status") == "enabled":
            print("âœ… ç¼“å­˜çŠ¶æ€æŸ¥è¯¢æˆåŠŸ")
            cached_files = cache_status.get("cached_files", 0)
            print(f"  ç¼“å­˜æ–‡ä»¶: {cached_files} ä¸ª")

            if cached_files > 0:
                # æ¸…ç†ç¼“å­˜
                clear_result = loader.clear_cache()
                if clear_result.get("status") == "all_cache_cleared":
                    print("âœ… ç¼“å­˜æ¸…ç†æˆåŠŸ")
                else:
                    print(f"âŒ ç¼“å­˜æ¸…ç†å¤±è´¥: {clear_result}")

        return True


def test_data_pipeline_integration():
    """æµ‹è¯•æ•°æ®ç®¡é“é›†æˆ"""
    print("\nğŸ”— æµ‹è¯•æ•°æ®ç®¡é“é›†æˆ...")

    # åˆ›å»ºä¸´æ—¶ç›®å½•
    with tempfile.TemporaryDirectory() as temp_dir:
        # é…ç½®æ•°æ®åŠ è½½å™¨
        config = {
            "enable_cache": True,
            "cache_directory": temp_dir,
            "enable_preprocessing": True,
            "create_unified_dataset": True,
            "use_fundamental": True,
            "use_technical": True,
            "use_sentiment": True,
            "use_macro": True,
        }

        loader = UnifiedDataLoader(config)

        # æµ‹è¯•å®Œæ•´çš„æ•°æ®ç®¡é“æµç¨‹
        print("\nğŸš€ æ‰§è¡Œå®Œæ•´æ•°æ®ç®¡é“æµç¨‹...")

        test_symbols = ["AAPL", "MSFT", "GOOGL"]
        objectives = ["growth", "value", "balanced"]

        results = {}

        for objective in objectives:
            print(f"\nğŸ“Š æµ‹è¯• {objective} ç­–ç•¥æ•°æ®ç®¡é“...")

            # åŠ è½½æ•°æ®
            data = loader.load_comprehensive_data(
                symbols=test_symbols, objective=objective, use_cache=False
            )

            if "error" not in data:
                results[objective] = data

                # æ£€æŸ¥æ•°æ®å®Œæ•´æ€§
                required_sections = [
                    "fundamental",
                    "technical",
                    "sentiment",
                    "macro",
                    "analysis",
                ]
                missing_sections = [
                    section for section in required_sections if section not in data
                ]

                if not missing_sections:
                    print(f"âœ… {objective} ç­–ç•¥æ•°æ®å®Œæ•´")
                else:
                    print(f"âš ï¸ {objective} ç­–ç•¥ç¼ºå°‘: {', '.join(missing_sections)}")

                # æ£€æŸ¥ç»Ÿä¸€æ•°æ®é›†
                if "unified_dataset" in data:
                    unified_df = data["unified_dataset"]
                    if isinstance(unified_df, pd.DataFrame) and not unified_df.empty:
                        print(f"  ç»Ÿä¸€æ•°æ®é›†: {len(unified_df)} è¡Œ")

                        # æ£€æŸ¥ç‰¹å¾è¦†ç›–
                        feature_types = {
                            "fundamental": [
                                col
                                for col in unified_df.columns
                                if col.startswith("fundamental_")
                            ],
                            "technical": [
                                col
                                for col in unified_df.columns
                                if col.startswith("technical_")
                            ],
                            "sentiment": [
                                col
                                for col in unified_df.columns
                                if col.startswith("sentiment_")
                            ],
                            "macro": [
                                col
                                for col in unified_df.columns
                                if col.startswith("macro_")
                            ],
                        }

                        for feature_type, features in feature_types.items():
                            print(f"    {feature_type} ç‰¹å¾: {len(features)} ä¸ª")

                # æ£€æŸ¥åˆ†æç»“æœ
                if "analysis" in data:
                    analysis = data["analysis"]

                    # æ£€æŸ¥å¸‚åœºåˆ†æ
                    if "market_analysis" in analysis:
                        market_analysis = analysis["market_analysis"]
                        if "tickers" in market_analysis:
                            selected_tickers = [
                                t["symbol"] for t in market_analysis["tickers"]
                            ]
                            print(f"  é€‰æ‹©è‚¡ç¥¨: {selected_tickers}")

                    # æ£€æŸ¥å®è§‚åˆ†æ
                    if "macro_analysis" in analysis:
                        macro_analysis = analysis["macro_analysis"]
                        if "investment_regime" in macro_analysis:
                            regime = macro_analysis["investment_regime"]
                            regime_type = regime.get("regime_type", "N/A")
                            confidence = regime.get("confidence", 0)
                            print(
                                f"  æŠ•èµ„ç¯å¢ƒ: {regime_type} (ç½®ä¿¡åº¦: {confidence:.2f})"
                            )

            else:
                print(f"âŒ {objective} ç­–ç•¥æ•°æ®åŠ è½½å¤±è´¥: {data['error']}")
                return False

        # å¯¹æ¯”ä¸åŒç­–ç•¥çš„ç»“æœ
        if len(results) > 1:
            print(f"\nğŸ“Š ç­–ç•¥å¯¹æ¯”åˆ†æ...")

            for obj1, obj2 in [
                ("growth", "value"),
                ("growth", "balanced"),
                ("value", "balanced"),
            ]:
                if obj1 in results and obj2 in results:
                    data1 = results[obj1]
                    data2 = results[obj2]

                    # å¯¹æ¯”é€‰è‚¡ç»“æœ
                    if "analysis" in data1 and "analysis" in data2:
                        analysis1 = data1["analysis"]
                        analysis2 = data2["analysis"]

                        if (
                            "market_analysis" in analysis1
                            and "market_analysis" in analysis2
                        ):
                            tickers1 = set(
                                t["symbol"]
                                for t in analysis1["market_analysis"].get("tickers", [])
                            )
                            tickers2 = set(
                                t["symbol"]
                                for t in analysis2["market_analysis"].get("tickers", [])
                            )

                            overlap = len(tickers1 & tickers2)
                            total_unique = len(tickers1 | tickers2)

                            print(
                                f"  {obj1} vs {obj2}: {overlap}/{total_unique} é‡å è‚¡ç¥¨"
                            )

        # æµ‹è¯•æ•°æ®æ›´æ–°
        print(f"\nğŸ”„ æµ‹è¯•æ•°æ®æ›´æ–°...")
        update_result = loader.update_cache(symbols=["AAPL"], force=False)

        if update_result.get("status") == "success":
            print("âœ… æ•°æ®æ›´æ–°æµ‹è¯•æˆåŠŸ")
        else:
            print(f"âŒ æ•°æ®æ›´æ–°æµ‹è¯•å¤±è´¥: {update_result}")

        # æµ‹è¯•æ”¯æŒçš„è‚¡ç¥¨åˆ—è¡¨
        supported_symbols = loader.get_supported_symbols()
        print(f"\nğŸ“‹ æ”¯æŒçš„è‚¡ç¥¨æ•°é‡: {len(supported_symbols)}")
        print(f"  ç¤ºä¾‹è‚¡ç¥¨: {supported_symbols[:5]}")

        return True


def test_error_handling():
    """æµ‹è¯•é”™è¯¯å¤„ç†"""
    print("\nğŸ›¡ï¸ æµ‹è¯•é”™è¯¯å¤„ç†...")

    # åˆ›å»ºä¸´æ—¶ç›®å½•
    with tempfile.TemporaryDirectory() as temp_dir:
        config = {
            "enable_cache": True,
            "cache_directory": temp_dir,
            "enable_preprocessing": True,
        }

        loader = UnifiedDataLoader(config)

        # æµ‹è¯•æ— æ•ˆè‚¡ç¥¨ä»£ç 
        print("\nğŸš« æµ‹è¯•æ— æ•ˆè‚¡ç¥¨ä»£ç ...")
        invalid_data = loader.load_comprehensive_data(
            symbols=["INVALID_SYMBOL"], objective="balanced", use_cache=False
        )

        # ç³»ç»Ÿåº”è¯¥èƒ½å¤„ç†æ— æ•ˆè‚¡ç¥¨è€Œä¸å´©æºƒ
        if "error" not in invalid_data:
            print("âœ… æ— æ•ˆè‚¡ç¥¨ä»£ç å¤„ç†æ­£å¸¸")
        else:
            print(f"âš ï¸ æ— æ•ˆè‚¡ç¥¨ä»£ç è§¦å‘é”™è¯¯: {invalid_data['error']}")

        # æµ‹è¯•æ— æ•ˆæ•°æ®ç±»å‹
        print("\nğŸš« æµ‹è¯•æ— æ•ˆæ•°æ®ç±»å‹...")
        try:
            invalid_type_data = loader.load_specific_data("invalid_type", ["AAPL"])
            if "error" in invalid_type_data:
                print("âœ… æ— æ•ˆæ•°æ®ç±»å‹é”™è¯¯å¤„ç†æ­£å¸¸")
            else:
                print("âš ï¸ æ— æ•ˆæ•°æ®ç±»å‹æœªè§¦å‘é¢„æœŸé”™è¯¯")
        except Exception as e:
            print(f"âœ… æ— æ•ˆæ•°æ®ç±»å‹è§¦å‘å¼‚å¸¸: {type(e).__name__}")

        # æµ‹è¯•ç©ºè‚¡ç¥¨åˆ—è¡¨
        print("\nğŸš« æµ‹è¯•ç©ºè‚¡ç¥¨åˆ—è¡¨...")
        empty_data = loader.load_comprehensive_data(
            symbols=[], objective="balanced", use_cache=False
        )

        if "error" in empty_data or not empty_data:
            print("âœ… ç©ºè‚¡ç¥¨åˆ—è¡¨å¤„ç†æ­£å¸¸")
        else:
            print("âš ï¸ ç©ºè‚¡ç¥¨åˆ—è¡¨å¤„ç†å¯èƒ½æœ‰é—®é¢˜")

        return True


if __name__ == "__main__":
    print("ğŸš€ æ•°æ®é¢„å¤„ç†ç®¡é“åŠŸèƒ½æµ‹è¯•")
    print("=" * 60)

    tests = [
        ("æ•°æ®é¢„å¤„ç†å™¨", test_data_preprocessor),
        ("ç»Ÿä¸€æ•°æ®åŠ è½½å™¨", test_unified_data_loader),
        ("æ•°æ®ç®¡é“é›†æˆ", test_data_pipeline_integration),
        ("é”™è¯¯å¤„ç†", test_error_handling),
    ]

    passed = 0
    total = len(tests)

    for test_name, test_func in tests:
        try:
            print(f"\nğŸ“‹ æ‰§è¡Œæµ‹è¯•: {test_name}")
            if test_func():
                print(f"âœ… {test_name} é€šè¿‡")
                passed += 1
            else:
                print(f"âŒ {test_name} å¤±è´¥")
        except Exception as e:
            print(f"âŒ {test_name} å¼‚å¸¸: {str(e)}")

    print("\n" + "=" * 60)
    print(f"ğŸ¯ æµ‹è¯•ç»“æœ: {passed}/{total} é€šè¿‡")

    if passed == total:
        print("ğŸ‰ æ‰€æœ‰æ•°æ®ç®¡é“åŠŸèƒ½æµ‹è¯•é€šè¿‡ï¼")
        print("âœ… æ•°æ®é¢„å¤„ç†ç®¡é“å·²å…¨é¢å»ºç«‹ï¼")
    else:
        print("âš ï¸ éƒ¨åˆ†æµ‹è¯•æœªé€šè¿‡ï¼Œéœ€è¦è¿›ä¸€æ­¥ä¼˜åŒ–")

    print("\nğŸ“ˆ æ•°æ®ç®¡é“æ–°å¢èƒ½åŠ›:")
    print("  âœ… å››æºæ•°æ®ç»Ÿä¸€é¢„å¤„ç†")
    print("  âœ… æ™ºèƒ½æ•°æ®è´¨é‡è¯„ä¼°")
    print("  âœ… è·¨å› å­ç‰¹å¾å·¥ç¨‹")
    print("  âœ… ç»Ÿä¸€æ•°æ®é›†æ„å»º")
    print("  âœ… é«˜æ•ˆæ•°æ®ç¼“å­˜æœºåˆ¶")
    print("  âœ… å®æ—¶æ•°æ®å¿«ç…§")
    print("  âœ… çµæ´»çš„æ•°æ®åŠ è½½é…ç½®")
    print("  âœ… å…¨é¢çš„é”™è¯¯å¤„ç†")
    print("  âœ… æ•°æ®è´¨é‡ç›‘æ§å’ŒæŠ¥å‘Š")
