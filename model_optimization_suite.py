#!/usr/bin/env python3
"""
StockSynergy æ¨¡å‹ä¼˜åŒ–å¥—ä»¶

ç³»ç»Ÿæ€§æå‡AIäº¤æ˜“æ¨¡å‹æ€§èƒ½ï¼ŒåŒ…æ‹¬ï¼š
1. ğŸ”§ ç‰¹å¾å·¥ç¨‹ä¼˜åŒ–
2. ğŸ¤– æ¨¡å‹æ¶æ„å‡çº§
3. ğŸ“Š æ•°æ®è´¨é‡æå‡
4. âš¡ è¶…å‚æ•°ç²¾è°ƒ
5. ğŸ§  é›†æˆå­¦ä¹ å¼ºåŒ–
"""

import sys
import os
import pandas as pd
import numpy as np
import pickle
from datetime import datetime, timedelta
from typing import Dict, List, Tuple, Any, Optional
import warnings
warnings.filterwarnings('ignore')

# æœºå™¨å­¦ä¹ æ ¸å¿ƒåº“
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.model_selection import GridSearchCV, TimeSeriesSplit, cross_val_score
from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler
from sklearn.feature_selection import SelectKBest, f_classif, RFE
from sklearn.metrics import classification_report, confusion_matrix
import xgboost as xgb
from scipy import stats

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

class AdvancedFeatureEngineer:
    """
    é«˜çº§ç‰¹å¾å·¥ç¨‹å™¨
    """

    def __init__(self):
        self.feature_importance = {}
        self.selected_features = []

    def create_technical_features(self, data: pd.DataFrame) -> pd.DataFrame:
        """åˆ›å»ºé«˜çº§æŠ€æœ¯æŒ‡æ ‡ç‰¹å¾"""
        print("ğŸ”§ åˆ›å»ºé«˜çº§æŠ€æœ¯æŒ‡æ ‡...")

        df = data.copy()

        # 1. ä»·æ ¼åŠ¨é‡ç‰¹å¾
        for period in [3, 7, 14, 21]:
            df[f'price_momentum_{period}'] = df['Returns'].rolling(period).mean()
            df[f'price_acceleration_{period}'] = df['Returns'].rolling(period).apply(
                lambda x: np.polyfit(range(len(x)), x, 1)[0] if len(x) == period else np.nan
            )

        # 2. æ³¢åŠ¨ç‡ç‰¹å¾
        for period in [5, 10, 20]:
            df[f'volatility_{period}'] = df['Returns'].rolling(period).std()
            df[f'volatility_ratio_{period}'] = df[f'volatility_{period}'] / df['Returns'].rolling(60).std()

        # 3. ç›¸å¯¹å¼ºå¼±æŒ‡æ ‡
        if 'RSI' in df.columns:
            df['rsi_oversold'] = (df['RSI'] < 30).astype(int)
            df['rsi_overbought'] = (df['RSI'] > 70).astype(int)
            df['rsi_momentum'] = df['RSI'].diff()

        # 4. å‡çº¿äº¤å‰ä¿¡å·
        if 'SMA_5' in df.columns and 'SMA_20' in df.columns:
            df['sma_cross_signal'] = np.where(df['SMA_5'] > df['SMA_20'], 1, -1)
            df['sma_divergence'] = (df['SMA_5'] - df['SMA_20']) / df['SMA_20']

        # 5. æˆäº¤é‡ç‰¹å¾
        if 'Volume_MA' in df.columns:
            df['volume_spike'] = (df['Volume_MA'] > df['Volume_MA'].rolling(20).mean() * 1.5).astype(int)
            df['volume_trend'] = df['Volume_MA'].rolling(5).apply(
                lambda x: np.polyfit(range(len(x)), x, 1)[0] if len(x) == 5 else np.nan
            )

        # 6. å¸‚åœºçŠ¶æ€ç‰¹å¾
        df['trend_strength'] = abs(df['Returns'].rolling(10).mean()) / df['Returns'].rolling(10).std()
        df['market_regime'] = pd.cut(df['Returns'].rolling(20).mean(),
                                    bins=[-np.inf, -0.01, 0.01, np.inf],
                                    labels=[0, 1, 2])  # ä¸‹è·Œã€éœ‡è¡ã€ä¸Šæ¶¨

        # 7. ç»Ÿè®¡ç‰¹å¾
        df['returns_skewness'] = df['Returns'].rolling(20).skew()
        df['returns_kurtosis'] = df['Returns'].rolling(20).kurt()

        print(f"  âœ… æ–°å¢ç‰¹å¾: {len(df.columns) - len(data.columns)}ä¸ª")
        return df.fillna(method='ffill').fillna(0)

    def create_cross_asset_features(self, stock_data: Dict[str, pd.DataFrame]) -> pd.DataFrame:
        """åˆ›å»ºè·¨èµ„äº§ç‰¹å¾"""
        print("ğŸŒ åˆ›å»ºè·¨èµ„äº§ç‰¹å¾...")

        all_features = []

        for symbol, data in stock_data.items():
            # åŸºç¡€ç‰¹å¾
            features = data.copy()
            features['symbol'] = symbol

            # ç›¸å¯¹è¡¨ç°ç‰¹å¾
            market_returns = pd.concat([d['Returns'] for d in stock_data.values()], axis=1).mean(axis=1)
            features['relative_performance'] = features['Returns'] - market_returns
            features['beta'] = features['Returns'].rolling(60).corr(market_returns)

            # æ’åç‰¹å¾
            all_returns = pd.concat([d['Returns'] for d in stock_data.values()], axis=1)
            features['return_rank'] = all_returns.rank(axis=1, pct=True)[symbol] if symbol in all_returns.columns else 0.5

            all_features.append(features)

        combined = pd.concat(all_features, ignore_index=True)
        print(f"  âœ… è·¨èµ„äº§ç‰¹å¾åˆ›å»ºå®Œæˆ")
        return combined

    def select_best_features(self, X: pd.DataFrame, y: pd.Series, n_features: int = 20) -> List[str]:
        """ç‰¹å¾é€‰æ‹©"""
        print(f"ğŸ¯ é€‰æ‹©æœ€ä½³ç‰¹å¾ (ç›®æ ‡: {n_features}ä¸ª)...")

        # ç§»é™¤éæ•°å€¼ç‰¹å¾
        numeric_features = X.select_dtypes(include=[np.number]).columns.tolist()
        X_numeric = X[numeric_features].fillna(0)

        # æ–¹æ³•1: ç»Ÿè®¡æ£€éªŒ
        selector1 = SelectKBest(score_func=f_classif, k=min(n_features*2, len(numeric_features)))
        selector1.fit(X_numeric, y)
        selected1 = X_numeric.columns[selector1.get_support()].tolist()

        # æ–¹æ³•2: é€’å½’ç‰¹å¾æ¶ˆé™¤ (ä½¿ç”¨Random Forest)
        rf = RandomForestClassifier(n_estimators=50, random_state=42)
        selector2 = RFE(rf, n_features_to_select=min(n_features, len(numeric_features)))
        selector2.fit(X_numeric, y)
        selected2 = X_numeric.columns[selector2.get_support()].tolist()

        # ç»“åˆä¸¤ç§æ–¹æ³•
        final_features = list(set(selected1 + selected2))[:n_features]

        print(f"  âœ… é€‰æ‹©ç‰¹å¾: {len(final_features)}ä¸ª")
        return final_features

class ModelOptimizer:
    """
    æ¨¡å‹ä¼˜åŒ–å™¨
    """

    def __init__(self):
        self.best_models = {}
        self.optimization_history = []

    def optimize_random_forest(self, X: pd.DataFrame, y: pd.Series) -> Dict[str, Any]:
        """ä¼˜åŒ–Random Forest"""
        print("ğŸŒ² ä¼˜åŒ–Random Forest...")

        # æ‰©å±•å‚æ•°ç½‘æ ¼
        param_grid = {
            'n_estimators': [100, 200, 300, 500],
            'max_depth': [10, 15, 20, 25, None],
            'min_samples_split': [2, 5, 10, 15],
            'min_samples_leaf': [1, 2, 4, 8],
            'max_features': ['sqrt', 'log2', 0.8, 0.9],
            'bootstrap': [True, False],
            'class_weight': ['balanced', 'balanced_subsample', None]
        }

        rf = RandomForestClassifier(random_state=42, n_jobs=-1)

        # æ—¶é—´åºåˆ—äº¤å‰éªŒè¯
        tscv = TimeSeriesSplit(n_splits=3)

        grid_search = GridSearchCV(
            rf, param_grid,
            cv=tscv,
            scoring='accuracy',
            n_jobs=-1,
            verbose=1
        )

        grid_search.fit(X, y)

        print(f"  âœ… æœ€ä½³å‚æ•°: {grid_search.best_params_}")
        print(f"  ğŸ“Š æœ€ä½³å¾—åˆ†: {grid_search.best_score_:.3f}")

        return {
            'model': grid_search.best_estimator_,
            'best_params': grid_search.best_params_,
            'best_score': grid_search.best_score_,
            'cv_results': grid_search.cv_results_
        }

    def optimize_xgboost(self, X: pd.DataFrame, y: pd.Series) -> Dict[str, Any]:
        """ä¼˜åŒ–XGBoost"""
        print("ğŸš€ ä¼˜åŒ–XGBoost...")

        param_grid = {
            'n_estimators': [100, 200, 300, 500],
            'max_depth': [4, 6, 8, 10],
            'learning_rate': [0.01, 0.05, 0.1, 0.2],
            'subsample': [0.6, 0.8, 0.9, 1.0],
            'colsample_bytree': [0.6, 0.8, 0.9, 1.0],
            'gamma': [0, 0.1, 0.5, 1.0],
            'reg_alpha': [0, 0.1, 0.5, 1.0],
            'reg_lambda': [0, 0.1, 0.5, 1.0]
        }

        xgb_model = xgb.XGBClassifier(
            random_state=42,
            n_jobs=-1,
            eval_metric='mlogloss'
        )

        tscv = TimeSeriesSplit(n_splits=3)

        grid_search = GridSearchCV(
            xgb_model, param_grid,
            cv=tscv,
            scoring='accuracy',
            n_jobs=-1,
            verbose=1
        )

        grid_search.fit(X, y)

        print(f"  âœ… æœ€ä½³å‚æ•°: {grid_search.best_params_}")
        print(f"  ğŸ“Š æœ€ä½³å¾—åˆ†: {grid_search.best_score_:.3f}")

        return {
            'model': grid_search.best_estimator_,
            'best_params': grid_search.best_params_,
            'best_score': grid_search.best_score_,
            'cv_results': grid_search.cv_results_
        }

    def create_gradient_boosting(self, X: pd.DataFrame, y: pd.Series) -> Dict[str, Any]:
        """åˆ›å»ºGradient Boostingæ¨¡å‹"""
        print("ğŸ“ˆ ä¼˜åŒ–Gradient Boosting...")

        param_grid = {
            'n_estimators': [100, 200, 300],
            'max_depth': [3, 5, 7, 10],
            'learning_rate': [0.01, 0.05, 0.1, 0.2],
            'subsample': [0.8, 0.9, 1.0],
            'max_features': ['sqrt', 'log2', 0.8]
        }

        gb = GradientBoostingClassifier(random_state=42)

        tscv = TimeSeriesSplit(n_splits=3)

        grid_search = GridSearchCV(
            gb, param_grid,
            cv=tscv,
            scoring='accuracy',
            n_jobs=-1,
            verbose=1
        )

        grid_search.fit(X, y)

        print(f"  âœ… æœ€ä½³å‚æ•°: {grid_search.best_params_}")
        print(f"  ğŸ“Š æœ€ä½³å¾—åˆ†: {grid_search.best_score_:.3f}")

        return {
            'model': grid_search.best_estimator_,
            'best_params': grid_search.best_params_,
            'best_score': grid_search.best_score_,
            'cv_results': grid_search.cv_results_
        }

class AdvancedEnsemble:
    """
    é«˜çº§é›†æˆå­¦ä¹ 
    """

    def __init__(self):
        self.models = {}
        self.weights = {}
        self.meta_model = None

    def create_stacking_ensemble(self, base_models: Dict[str, Any], X: pd.DataFrame, y: pd.Series):
        """åˆ›å»ºStackingé›†æˆ"""
        print("ğŸ­ åˆ›å»ºStackingé›†æˆ...")

        from sklearn.ensemble import StackingClassifier
        from sklearn.linear_model import LogisticRegression

        estimators = [(name, model['model']) for name, model in base_models.items()]

        stacking_clf = StackingClassifier(
            estimators=estimators,
            final_estimator=LogisticRegression(random_state=42),
            cv=TimeSeriesSplit(n_splits=3),
            n_jobs=-1
        )

        stacking_clf.fit(X, y)

        # äº¤å‰éªŒè¯è¯„ä¼°
        tscv = TimeSeriesSplit(n_splits=3)
        cv_scores = cross_val_score(stacking_clf, X, y, cv=tscv, scoring='accuracy')

        print(f"  âœ… Stackingå‡†ç¡®ç‡: {cv_scores.mean():.3f} (+/- {cv_scores.std() * 2:.3f})")

        return {
            'model': stacking_clf,
            'cv_scores': cv_scores,
            'mean_score': cv_scores.mean()
        }

    def create_voting_ensemble(self, base_models: Dict[str, Any], X: pd.DataFrame, y: pd.Series):
        """åˆ›å»ºVotingé›†æˆ"""
        print("ğŸ—³ï¸ åˆ›å»ºVotingé›†æˆ...")

        from sklearn.ensemble import VotingClassifier

        estimators = [(name, model['model']) for name, model in base_models.items()]

        # ç¡¬æŠ•ç¥¨
        hard_voting = VotingClassifier(estimators=estimators, voting='hard', n_jobs=-1)
        hard_voting.fit(X, y)

        # è½¯æŠ•ç¥¨
        soft_voting = VotingClassifier(estimators=estimators, voting='soft', n_jobs=-1)
        soft_voting.fit(X, y)

        # è¯„ä¼°
        tscv = TimeSeriesSplit(n_splits=3)
        hard_scores = cross_val_score(hard_voting, X, y, cv=tscv, scoring='accuracy')
        soft_scores = cross_val_score(soft_voting, X, y, cv=tscv, scoring='accuracy')

        print(f"  âœ… ç¡¬æŠ•ç¥¨å‡†ç¡®ç‡: {hard_scores.mean():.3f}")
        print(f"  âœ… è½¯æŠ•ç¥¨å‡†ç¡®ç‡: {soft_scores.mean():.3f}")

        best_voting = soft_voting if soft_scores.mean() > hard_scores.mean() else hard_voting
        best_scores = soft_scores if soft_scores.mean() > hard_scores.mean() else hard_scores

        return {
            'model': best_voting,
            'cv_scores': best_scores,
            'mean_score': best_scores.mean(),
            'type': 'soft' if soft_scores.mean() > hard_scores.mean() else 'hard'
        }

class DataQualityEnhancer:
    """
    æ•°æ®è´¨é‡å¢å¼ºå™¨
    """

    def __init__(self):
        pass

    def clean_outliers(self, data: pd.DataFrame, method='iqr') -> pd.DataFrame:
        """æ¸…ç†å¼‚å¸¸å€¼"""
        print(f"ğŸ§¹ æ¸…ç†å¼‚å¸¸å€¼ (æ–¹æ³•: {method})...")

        df = data.copy()
        numeric_columns = df.select_dtypes(include=[np.number]).columns

        for col in numeric_columns:
            if method == 'iqr':
                Q1 = df[col].quantile(0.25)
                Q3 = df[col].quantile(0.75)
                IQR = Q3 - Q1
                lower_bound = Q1 - 1.5 * IQR
                upper_bound = Q3 + 1.5 * IQR
                df[col] = df[col].clip(lower_bound, upper_bound)

            elif method == 'zscore':
                z_scores = np.abs(stats.zscore(df[col].fillna(df[col].mean())))
                df[col] = df[col].where(z_scores < 3, df[col].median())

        print(f"  âœ… å¼‚å¸¸å€¼æ¸…ç†å®Œæˆ")
        return df

    def balance_dataset(self, X: pd.DataFrame, y: pd.Series) -> Tuple[pd.DataFrame, pd.Series]:
        """å¹³è¡¡æ•°æ®é›†"""
        print("âš–ï¸ å¹³è¡¡æ•°æ®é›†...")

        from sklearn.utils import resample

        # åˆå¹¶æ•°æ®
        data = X.copy()
        data['target'] = y

        # åˆ†ç¦»å„ç±»åˆ«
        class_groups = data.groupby('target')

        # æ‰¾åˆ°æœ€å¤§ç±»åˆ«çš„æ ·æœ¬æ•°
        max_size = class_groups.size().max()

        # å¯¹å°‘æ•°ç±»è¿›è¡Œä¸Šé‡‡æ ·
        balanced_groups = []
        for class_label, group in class_groups:
            if len(group) < max_size:
                # ä¸Šé‡‡æ ·
                upsampled = resample(group,
                                   replace=True,
                                   n_samples=max_size,
                                   random_state=42)
                balanced_groups.append(upsampled)
            else:
                balanced_groups.append(group)

        # åˆå¹¶å¹³è¡¡åçš„æ•°æ®
        balanced_data = pd.concat(balanced_groups, ignore_index=True)

        X_balanced = balanced_data.drop('target', axis=1)
        y_balanced = balanced_data['target']

        print(f"  âœ… æ•°æ®å¹³è¡¡å®Œæˆ: {len(X_balanced)}ä¸ªæ ·æœ¬")
        print(f"  ğŸ“Š ç±»åˆ«åˆ†å¸ƒ: {y_balanced.value_counts().to_dict()}")

        return X_balanced, y_balanced

def run_comprehensive_optimization():
    """è¿è¡Œç»¼åˆä¼˜åŒ–"""
    print("ğŸš€ StockSynergy æ¨¡å‹ç»¼åˆä¼˜åŒ–")
    print("=" * 60)

    # 1. åŠ è½½æ•°æ®
    print("\nğŸ“Š åŠ è½½å†å²æ•°æ®...")
    try:
        with open('data/historical/complete_dataset_2020-01-01_2024-01-01.pkl', 'rb') as f:
            dataset = pickle.load(f)

        X_raw = dataset['features']
        y_raw = dataset['labels']

        print(f"  âœ… åŸå§‹æ•°æ®: {len(X_raw)}æ ·æœ¬, {len(X_raw.columns)}ç‰¹å¾")

    except FileNotFoundError:
        print("  âŒ æœªæ‰¾åˆ°å†å²æ•°æ®æ–‡ä»¶ï¼Œè¯·å…ˆè¿è¡Œ collect_historical_data.py")
        return

    # 2. æ•°æ®è´¨é‡å¢å¼º
    print("\nğŸ”§ æ•°æ®è´¨é‡å¢å¼º...")
    enhancer = DataQualityEnhancer()

    X_clean = enhancer.clean_outliers(X_raw)
    X_balanced, y_balanced = enhancer.balance_dataset(X_clean, y_raw)

    # 3. é«˜çº§ç‰¹å¾å·¥ç¨‹
    print("\nâš™ï¸ é«˜çº§ç‰¹å¾å·¥ç¨‹...")
    feature_engineer = AdvancedFeatureEngineer()

    # é‡æ–°æ„é€ å¸¦æœ‰æ›´å¤šç‰¹å¾çš„æ•°æ®
    enhanced_features = feature_engineer.create_technical_features(X_balanced)

    # ç‰¹å¾é€‰æ‹©
    best_features = feature_engineer.select_best_features(enhanced_features, y_balanced, n_features=30)
    X_selected = enhanced_features[best_features]

    print(f"  âœ… æœ€ç»ˆç‰¹å¾é›†: {len(best_features)}ä¸ªç‰¹å¾")

    # 4. æ•°æ®åˆ†å‰²
    print("\nâœ‚ï¸ æ•°æ®åˆ†å‰²...")
    split_idx = int(len(X_selected) * 0.8)
    X_train, X_test = X_selected[:split_idx], X_selected[split_idx:]
    y_train, y_test = y_balanced[:split_idx], y_balanced[split_idx:]

    # 5. æ¨¡å‹ä¼˜åŒ–
    print("\nğŸ¤– æ¨¡å‹ä¼˜åŒ–...")
    optimizer = ModelOptimizer()

    # ä¼˜åŒ–å„ä¸ªæ¨¡å‹
    rf_result = optimizer.optimize_random_forest(X_train, y_train)
    xgb_result = optimizer.optimize_xgboost(X_train, y_train)
    gb_result = optimizer.create_gradient_boosting(X_train, y_train)

    base_models = {
        'random_forest': rf_result,
        'xgboost': xgb_result,
        'gradient_boosting': gb_result
    }

    # 6. é«˜çº§é›†æˆ
    print("\nğŸ­ é«˜çº§é›†æˆå­¦ä¹ ...")
    ensemble = AdvancedEnsemble()

    stacking_result = ensemble.create_stacking_ensemble(base_models, X_train, y_train)
    voting_result = ensemble.create_voting_ensemble(base_models, X_train, y_train)

    # 7. æœ€ç»ˆè¯„ä¼°
    print("\nğŸ“Š æœ€ç»ˆæ¨¡å‹è¯„ä¼°...")

    all_models = {
        'Random Forest': rf_result,
        'XGBoost': xgb_result,
        'Gradient Boosting': gb_result,
        'Stacking Ensemble': stacking_result,
        'Voting Ensemble': voting_result
    }

    best_model_name = max(all_models.keys(), key=lambda x: all_models[x]['mean_score'])
    best_model = all_models[best_model_name]

    # æµ‹è¯•é›†è¯„ä¼°
    test_predictions = best_model['model'].predict(X_test)
    test_accuracy = np.mean(test_predictions == y_test)

    print(f"\nğŸ† æœ€ä½³æ¨¡å‹: {best_model_name}")
    print(f"ğŸ“Š äº¤å‰éªŒè¯å¾—åˆ†: {best_model['mean_score']:.3f}")
    print(f"ğŸ“Š æµ‹è¯•é›†å‡†ç¡®ç‡: {test_accuracy:.3f}")

    # 8. ç”Ÿæˆä¼˜åŒ–æŠ¥å‘Š
    print("\nğŸ“‹ ç”Ÿæˆä¼˜åŒ–æŠ¥å‘Š...")

    report_lines = [
        "=" * 80,
        "StockSynergy æ¨¡å‹ä¼˜åŒ–æŠ¥å‘Š",
        "=" * 80,
        f"ä¼˜åŒ–å®Œæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}",
        "",
        "ğŸ“Š æ•°æ®ä¼˜åŒ–ç»“æœ:",
        f"- åŸå§‹æ ·æœ¬: {len(X_raw):,}",
        f"- å¹³è¡¡åæ ·æœ¬: {len(X_balanced):,}",
        f"- ç‰¹å¾å·¥ç¨‹: {len(X_raw.columns)} â†’ {len(best_features)}",
        "",
        "ğŸ¤– æ¨¡å‹æ€§èƒ½å¯¹æ¯”:",
        "-" * 60,
        f"{'æ¨¡å‹':<20} {'CVå¾—åˆ†':<12} {'æµ‹è¯•å‡†ç¡®ç‡':<12}",
        "-" * 60
    ]

    for name, result in all_models.items():
        if name == best_model_name:
            test_acc = test_accuracy
        else:
            try:
                test_pred = result['model'].predict(X_test)
                test_acc = np.mean(test_pred == y_test)
            except:
                test_acc = 0.0

        report_lines.append(f"{name:<20} {result['mean_score']:<12.3f} {test_acc:<12.3f}")

    report_lines.extend([
        "-" * 60,
        "",
        f"ğŸ† æœ€ä½³æ¨¡å‹: {best_model_name}",
        f"ğŸ“ˆ æ€§èƒ½æå‡: {test_accuracy:.1%} (ç›®æ ‡: 65%+)",
        f"ğŸ¯ ä¼˜åŒ–çŠ¶æ€: {'âœ… è¾¾æ ‡' if test_accuracy >= 0.65 else 'âš ï¸ éœ€è¿›ä¸€æ­¥ä¼˜åŒ–'}",
        "",
        "ğŸ”§ å…³é”®ä¼˜åŒ–ç‚¹:",
        "â€¢ æ•°æ®å¹³è¡¡å’Œå¼‚å¸¸å€¼å¤„ç†",
        "â€¢ é«˜çº§æŠ€æœ¯æŒ‡æ ‡ç‰¹å¾å·¥ç¨‹",
        "â€¢ å¤šæ¨¡å‹é›†æˆå­¦ä¹ ",
        "â€¢ è¶…å‚æ•°ç½‘æ ¼æœç´¢ä¼˜åŒ–",
        "",
        "=" * 80
    ])

    # ä¿å­˜æŠ¥å‘Š
    report_content = "\n".join(report_lines)
    report_file = f"model_optimization_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt"

    with open(report_file, 'w', encoding='utf-8') as f:
        f.write(report_content)

    print(f"ğŸ“„ ä¼˜åŒ–æŠ¥å‘Šå·²ä¿å­˜: {report_file}")

    # 9. ä¿å­˜æœ€ä½³æ¨¡å‹
    model_file = f"best_model_{best_model_name.lower().replace(' ', '_')}_{datetime.now().strftime('%Y%m%d')}.pkl"

    model_package = {
        'model': best_model['model'],
        'feature_names': best_features,
        'model_name': best_model_name,
        'performance': {
            'cv_score': best_model['mean_score'],
            'test_accuracy': test_accuracy
        },
        'optimization_date': datetime.now().isoformat()
    }

    with open(model_file, 'wb') as f:
        pickle.dump(model_package, f)

    print(f"ğŸ’¾ æœ€ä½³æ¨¡å‹å·²ä¿å­˜: {model_file}")

    return {
        'best_model': best_model,
        'best_model_name': best_model_name,
        'test_accuracy': test_accuracy,
        'feature_names': best_features
    }

if __name__ == "__main__":
    try:
        results = run_comprehensive_optimization()

        print(f"\nğŸ‰ æ¨¡å‹ä¼˜åŒ–å®Œæˆï¼")
        print(f"ğŸ† æœ€ä½³æ¨¡å‹: {results['best_model_name']}")
        print(f"ğŸ“Š æµ‹è¯•å‡†ç¡®ç‡: {results['test_accuracy']:.1%}")

        if results['test_accuracy'] >= 0.65:
            print("âœ… æˆåŠŸè¾¾åˆ°65%+ç›®æ ‡å‡†ç¡®ç‡ï¼")
        else:
            print("âš ï¸ æœªè¾¾åˆ°ç›®æ ‡ï¼Œå»ºè®®:")
            print("  â€¢ æ”¶é›†æ›´å¤šå†å²æ•°æ®")
            print("  â€¢ åŠ å…¥æ›´å¤šå¤–éƒ¨ç‰¹å¾")
            print("  â€¢ å°è¯•æ·±åº¦å­¦ä¹ æ¨¡å‹")

    except Exception as e:
        print(f"âŒ ä¼˜åŒ–å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()