#!/usr/bin/env python3
"""
StockSynergy å¿«é€Ÿæ¨¡å‹ä¼˜åŒ– - 5åˆ†é’Ÿç‰ˆæœ¬

ä¸“æ³¨äºå…³é”®ä¼˜åŒ–ç‚¹ï¼Œå¿«é€Ÿæå‡æ¨¡å‹æ€§èƒ½ï¼š
1. âš¡ ç²¾ç®€å‚æ•°ç½‘æ ¼ (å‡å°‘90%æœç´¢ç©ºé—´)
2. ğŸ¯ æ™ºèƒ½ç‰¹å¾é€‰æ‹© (Top 15ç‰¹å¾)
3. ğŸ”§ å¿«é€Ÿé›†æˆå­¦ä¹ 
4. ğŸ“Š é«˜æ•ˆè¯„ä¼°æµç¨‹

ç›®æ ‡: 5åˆ†é’Ÿå†…å®Œæˆï¼Œå‡†ç¡®ç‡æå‡è‡³65%+
"""

import sys
import os
import pandas as pd
import numpy as np
import pickle
from datetime import datetime
from typing import Dict, List, Tuple, Any
import warnings
warnings.filterwarnings('ignore')

from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier
from sklearn.model_selection import RandomizedSearchCV, TimeSeriesSplit
from sklearn.preprocessing import StandardScaler
from sklearn.feature_selection import SelectKBest, f_classif
from sklearn.metrics import accuracy_score, classification_report
import xgboost as xgb

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

class QuickOptimizer:
    """
    å¿«é€Ÿä¼˜åŒ–å™¨ - 5åˆ†é’Ÿæå®š
    """

    def __init__(self):
        self.results = {}

    def quick_feature_engineering(self, data: pd.DataFrame) -> pd.DataFrame:
        """å¿«é€Ÿç‰¹å¾å·¥ç¨‹ - åªæ·»åŠ æœ€æœ‰æ•ˆçš„ç‰¹å¾"""
        print("âš¡ å¿«é€Ÿç‰¹å¾å·¥ç¨‹...")

        df = data.copy()

        # åªæ·»åŠ æœ€æœ‰æ•ˆçš„æŠ€æœ¯æŒ‡æ ‡
        if 'Returns' in df.columns:
            # çŸ­æœŸåŠ¨é‡
            df['momentum_3'] = df['Returns'].rolling(3).mean()
            df['momentum_7'] = df['Returns'].rolling(7).mean()

            # æ³¢åŠ¨ç‡
            df['volatility_5'] = df['Returns'].rolling(5).std()
            df['volatility_20'] = df['Returns'].rolling(20).std()

            # ä»·æ ¼åŠ é€Ÿåº¦
            df['acceleration'] = df['Returns'].diff()

        # RSIç›¸å…³
        if 'RSI' in df.columns:
            df['rsi_signal'] = np.where(df['RSI'] < 30, 2, np.where(df['RSI'] > 70, 0, 1))
            df['rsi_momentum'] = df['RSI'].diff()

        # å‡çº¿äº¤å‰
        if 'SMA_5' in df.columns and 'SMA_20' in df.columns:
            df['sma_ratio'] = df['SMA_5'] / df['SMA_20']
            df['sma_signal'] = np.where(df['SMA_5'] > df['SMA_20'], 1, 0)

        # æˆäº¤é‡
        if 'Volume_MA' in df.columns:
            df['volume_signal'] = np.where(df['Volume_MA'] > df['Volume_MA'].rolling(10).mean(), 1, 0)

        print(f"  âœ… æ–°å¢ç‰¹å¾: {len(df.columns) - len(data.columns)}ä¸ª")
        return df.fillna(method='ffill').fillna(0)

    def smart_feature_selection(self, X: pd.DataFrame, y: pd.Series, n_features: int = 15) -> List[str]:
        """æ™ºèƒ½ç‰¹å¾é€‰æ‹© - å¿«é€Ÿé€‰å‡ºæœ€é‡è¦çš„ç‰¹å¾"""
        print(f"ğŸ¯ æ™ºèƒ½ç‰¹å¾é€‰æ‹© (Top {n_features})...")

        # ç§»é™¤éæ•°å€¼ç‰¹å¾
        numeric_features = X.select_dtypes(include=[np.number]).columns.tolist()
        X_numeric = X[numeric_features].fillna(0)

        # ä½¿ç”¨Fæ£€éªŒå¿«é€Ÿé€‰æ‹©
        selector = SelectKBest(score_func=f_classif, k=min(n_features, len(numeric_features)))
        selector.fit(X_numeric, y)

        selected_features = X_numeric.columns[selector.get_support()].tolist()

        # æ˜¾ç¤ºç‰¹å¾é‡è¦æ€§
        feature_scores = selector.scores_[selector.get_support()]
        feature_importance = dict(zip(selected_features, feature_scores))
        sorted_features = sorted(feature_importance.items(), key=lambda x: x[1], reverse=True)

        print("  ğŸ”¥ Top 10é‡è¦ç‰¹å¾:")
        for feature, score in sorted_features[:10]:
            print(f"    â€¢ {feature}: {score:.1f}")

        return selected_features

    def balance_data_quick(self, X: pd.DataFrame, y: pd.Series) -> Tuple[pd.DataFrame, pd.Series]:
        """å¿«é€Ÿæ•°æ®å¹³è¡¡"""
        print("âš–ï¸ å¿«é€Ÿæ•°æ®å¹³è¡¡...")

        from sklearn.utils import resample

        # åˆå¹¶æ•°æ®
        data = X.copy()
        data['target'] = y

        # æ‰¾åˆ°æœ€å°ç±»åˆ«æ•°é‡ï¼ˆé¿å…æ•°æ®è†¨èƒ€ï¼‰
        min_size = data['target'].value_counts().min()
        target_size = min(min_size * 2, 8000)  # é™åˆ¶æœ€å¤§æ ·æœ¬æ•°

        # å¹³è¡¡å„ç±»åˆ«
        balanced_groups = []
        for class_label in data['target'].unique():
            class_data = data[data['target'] == class_label]

            if len(class_data) > target_size:
                # ä¸‹é‡‡æ ·
                sampled = resample(class_data,
                                 replace=False,
                                 n_samples=target_size,
                                 random_state=42)
            else:
                # ä¸Šé‡‡æ ·
                sampled = resample(class_data,
                                 replace=True,
                                 n_samples=target_size,
                                 random_state=42)

            balanced_groups.append(sampled)

        balanced_data = pd.concat(balanced_groups, ignore_index=True)

        X_balanced = balanced_data.drop('target', axis=1)
        y_balanced = balanced_data['target']

        print(f"  âœ… å¹³è¡¡å®Œæˆ: {len(X_balanced)}æ ·æœ¬")
        print(f"  ğŸ“Š ç±»åˆ«åˆ†å¸ƒ: {y_balanced.value_counts().to_dict()}")

        return X_balanced, y_balanced

    def optimize_random_forest_quick(self, X: pd.DataFrame, y: pd.Series) -> Dict[str, Any]:
        """å¿«é€Ÿä¼˜åŒ–Random Forest"""
        print("ğŸŒ² å¿«é€Ÿä¼˜åŒ–Random Forest...")

        # ç²¾ç®€å‚æ•°ç½‘æ ¼ (åªæœç´¢å…³é”®å‚æ•°)
        param_distributions = {
            'n_estimators': [100, 200, 300],
            'max_depth': [10, 15, 20, None],
            'min_samples_split': [2, 5, 10],
            'min_samples_leaf': [1, 2, 4],
            'max_features': ['sqrt', 'log2']
        }

        rf = RandomForestClassifier(random_state=42, n_jobs=-1)

        # éšæœºæœç´¢ (æ¯”ç½‘æ ¼æœç´¢å¿«å¾ˆå¤š)
        random_search = RandomizedSearchCV(
            rf, param_distributions,
            n_iter=20,  # åªè¯•20ä¸ªç»„åˆ
            cv=3,  # åªåš3æŠ˜éªŒè¯
            scoring='accuracy',
            n_jobs=-1,
            random_state=42
        )

        random_search.fit(X, y)

        print(f"  âœ… æœ€ä½³å‚æ•°: {random_search.best_params_}")
        print(f"  ğŸ“Š æœ€ä½³å¾—åˆ†: {random_search.best_score_:.3f}")

        return {
            'model': random_search.best_estimator_,
            'best_score': random_search.best_score_,
            'best_params': random_search.best_params_
        }

    def optimize_xgboost_quick(self, X: pd.DataFrame, y: pd.Series) -> Dict[str, Any]:
        """å¿«é€Ÿä¼˜åŒ–XGBoost"""
        print("ğŸš€ å¿«é€Ÿä¼˜åŒ–XGBoost...")

        param_distributions = {
            'n_estimators': [100, 200, 300],
            'max_depth': [4, 6, 8],
            'learning_rate': [0.05, 0.1, 0.2],
            'subsample': [0.8, 0.9, 1.0],
            'colsample_bytree': [0.8, 0.9, 1.0]
        }

        xgb_model = xgb.XGBClassifier(
            random_state=42,
            n_jobs=-1,
            eval_metric='mlogloss'
        )

        random_search = RandomizedSearchCV(
            xgb_model, param_distributions,
            n_iter=20,
            cv=3,
            scoring='accuracy',
            n_jobs=-1,
            random_state=42
        )

        random_search.fit(X, y)

        print(f"  âœ… æœ€ä½³å‚æ•°: {random_search.best_params_}")
        print(f"  ğŸ“Š æœ€ä½³å¾—åˆ†: {random_search.best_score_:.3f}")

        return {
            'model': random_search.best_estimator_,
            'best_score': random_search.best_score_,
            'best_params': random_search.best_params_
        }

    def create_gradient_boosting_quick(self, X: pd.DataFrame, y: pd.Series) -> Dict[str, Any]:
        """å¿«é€Ÿåˆ›å»ºGradient Boosting"""
        print("ğŸ“ˆ å¿«é€Ÿä¼˜åŒ–Gradient Boosting...")

        param_distributions = {
            'n_estimators': [100, 200],
            'max_depth': [3, 5, 7],
            'learning_rate': [0.05, 0.1, 0.2],
            'subsample': [0.8, 0.9]
        }

        gb = GradientBoostingClassifier(random_state=42)

        random_search = RandomizedSearchCV(
            gb, param_distributions,
            n_iter=15,
            cv=3,
            scoring='accuracy',
            n_jobs=-1,
            random_state=42
        )

        random_search.fit(X, y)

        print(f"  âœ… æœ€ä½³å‚æ•°: {random_search.best_params_}")
        print(f"  ğŸ“Š æœ€ä½³å¾—åˆ†: {random_search.best_score_:.3f}")

        return {
            'model': random_search.best_estimator_,
            'best_score': random_search.best_score_,
            'best_params': random_search.best_params_
        }

    def create_quick_ensemble(self, models: Dict[str, Any], X: pd.DataFrame, y: pd.Series) -> Dict[str, Any]:
        """å¿«é€Ÿåˆ›å»ºé›†æˆæ¨¡å‹"""
        print("ğŸ­ å¿«é€Ÿé›†æˆå­¦ä¹ ...")

        # ç®€å•æŠ•ç¥¨é›†æˆ
        estimators = [(name, model['model']) for name, model in models.items()]

        voting_clf = VotingClassifier(
            estimators=estimators,
            voting='soft',  # è½¯æŠ•ç¥¨é€šå¸¸æ›´å¥½
            n_jobs=-1
        )

        voting_clf.fit(X, y)

        # ç®€å•äº¤å‰éªŒè¯
        from sklearn.model_selection import cross_val_score
        cv_scores = cross_val_score(voting_clf, X, y, cv=3, scoring='accuracy')

        print(f"  âœ… é›†æˆå‡†ç¡®ç‡: {cv_scores.mean():.3f} (+/- {cv_scores.std() * 2:.3f})")

        return {
            'model': voting_clf,
            'best_score': cv_scores.mean(),
            'cv_scores': cv_scores
        }

def run_quick_optimization():
    """è¿è¡Œå¿«é€Ÿä¼˜åŒ– - 5åˆ†é’Ÿç‰ˆæœ¬"""
    print("âš¡ StockSynergy å¿«é€Ÿæ¨¡å‹ä¼˜åŒ– (5åˆ†é’Ÿç‰ˆæœ¬)")
    print("=" * 60)

    start_time = datetime.now()

    # 1. åŠ è½½æ•°æ®
    print("\nğŸ“Š åŠ è½½å†å²æ•°æ®...")
    try:
        with open('data/historical/complete_dataset_2020-01-01_2024-01-01.pkl', 'rb') as f:
            dataset = pickle.load(f)

        X_raw = dataset['features']
        y_raw = dataset['labels']

        print(f"  âœ… åŸå§‹æ•°æ®: {len(X_raw)}æ ·æœ¬, {len(X_raw.columns)}ç‰¹å¾")

    except FileNotFoundError:
        print("  âŒ æœªæ‰¾åˆ°å†å²æ•°æ®æ–‡ä»¶ï¼Œè¯·å…ˆè¿è¡Œ collect_historical_data.py")
        return

    # 2. å¿«é€Ÿä¼˜åŒ–å™¨
    optimizer = QuickOptimizer()

    # 3. å¿«é€Ÿç‰¹å¾å·¥ç¨‹
    print("\nâš¡ å¿«é€Ÿç‰¹å¾å·¥ç¨‹...")
    X_enhanced = optimizer.quick_feature_engineering(X_raw)

    # 4. æ™ºèƒ½ç‰¹å¾é€‰æ‹©
    selected_features = optimizer.smart_feature_selection(X_enhanced, y_raw, n_features=15)
    X_selected = X_enhanced[selected_features]

    # 5. å¿«é€Ÿæ•°æ®å¹³è¡¡
    X_balanced, y_balanced = optimizer.balance_data_quick(X_selected, y_raw)

    # 6. æ•°æ®åˆ†å‰²
    print("\nâœ‚ï¸ æ•°æ®åˆ†å‰²...")
    split_idx = int(len(X_balanced) * 0.8)
    X_train, X_test = X_balanced[:split_idx], X_balanced[split_idx:]
    y_train, y_test = y_balanced[:split_idx], y_balanced[split_idx:]

    print(f"  ğŸ“Š è®­ç»ƒé›†: {len(X_train)}æ ·æœ¬")
    print(f"  ğŸ“Š æµ‹è¯•é›†: {len(X_test)}æ ·æœ¬")

    # 7. å¿«é€Ÿæ¨¡å‹ä¼˜åŒ–
    print("\nğŸš€ å¿«é€Ÿæ¨¡å‹ä¼˜åŒ–...")

    rf_result = optimizer.optimize_random_forest_quick(X_train, y_train)
    xgb_result = optimizer.optimize_xgboost_quick(X_train, y_train)
    gb_result = optimizer.create_gradient_boosting_quick(X_train, y_train)

    base_models = {
        'random_forest': rf_result,
        'xgboost': xgb_result,
        'gradient_boosting': gb_result
    }

    # 8. å¿«é€Ÿé›†æˆ
    ensemble_result = optimizer.create_quick_ensemble(base_models, X_train, y_train)

    # 9. æœ€ç»ˆè¯„ä¼°
    print("\nğŸ“Š æœ€ç»ˆè¯„ä¼°...")

    all_models = {**base_models, 'ensemble': ensemble_result}

    # æ‰¾å‡ºæœ€ä½³æ¨¡å‹
    best_model_name = max(all_models.keys(), key=lambda x: all_models[x]['best_score'])
    best_model = all_models[best_model_name]

    # æµ‹è¯•é›†è¯„ä¼°
    test_predictions = best_model['model'].predict(X_test)
    test_accuracy = accuracy_score(y_test, test_predictions)

    # è®¡ç®—è¿è¡Œæ—¶é—´
    end_time = datetime.now()
    total_time = (end_time - start_time).total_seconds()

    print(f"\nğŸ† ä¼˜åŒ–ç»“æœ:")
    print(f"  â±ï¸ æ€»ç”¨æ—¶: {total_time:.1f}ç§’ ({total_time/60:.1f}åˆ†é’Ÿ)")
    print(f"  ğŸ¯ æœ€ä½³æ¨¡å‹: {best_model_name}")
    print(f"  ğŸ“Š äº¤å‰éªŒè¯å¾—åˆ†: {best_model['best_score']:.3f}")
    print(f"  ğŸ“Š æµ‹è¯•é›†å‡†ç¡®ç‡: {test_accuracy:.3f}")

    # æ€§èƒ½æå‡åˆ†æ
    original_accuracy = 0.52  # åŸå§‹æœ€ä½³éªŒè¯å‡†ç¡®ç‡
    improvement = test_accuracy - original_accuracy

    print(f"\nğŸ“ˆ æ€§èƒ½æå‡åˆ†æ:")
    print(f"  ğŸ“Š åŸå§‹å‡†ç¡®ç‡: {original_accuracy:.1%}")
    print(f"  ğŸ“Š ä¼˜åŒ–åå‡†ç¡®ç‡: {test_accuracy:.1%}")
    print(f"  ğŸš€ æ€§èƒ½æå‡: {improvement:+.1%}")

    success = test_accuracy >= 0.65
    print(f"  ğŸ¯ ç›®æ ‡è¾¾æˆ: {'âœ… æ˜¯' if success else 'âŒ å¦'} (ç›®æ ‡: 65%+)")

    # 10. ç”Ÿæˆå¿«é€ŸæŠ¥å‘Š
    print(f"\nğŸ“‹ ç”Ÿæˆå¿«é€ŸæŠ¥å‘Š...")

    report_lines = [
        "=" * 60,
        "StockSynergy å¿«é€Ÿæ¨¡å‹ä¼˜åŒ–æŠ¥å‘Š",
        "=" * 60,
        f"ä¼˜åŒ–æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}",
        f"æ€»ç”¨æ—¶: {total_time:.1f}ç§’",
        "",
        "ğŸ“Š ä¼˜åŒ–ç»“æœ:",
        f"- æœ€ä½³æ¨¡å‹: {best_model_name}",
        f"- æµ‹è¯•å‡†ç¡®ç‡: {test_accuracy:.1%}",
        f"- æ€§èƒ½æå‡: {improvement:+.1%}",
        f"- ç›®æ ‡è¾¾æˆ: {'âœ…' if success else 'âŒ'}",
        "",
        "ğŸ”§ ä¼˜åŒ–æŠ€æœ¯:",
        "- æ™ºèƒ½ç‰¹å¾é€‰æ‹© (Top 15)",
        "- éšæœºæœç´¢ä¼˜åŒ– (vs ç½‘æ ¼æœç´¢)",
        "- å¿«é€Ÿæ•°æ®å¹³è¡¡",
        "- é›†æˆå­¦ä¹ ",
        "",
        "âš¡ æ•ˆç‡æå‡:",
        f"- æœç´¢ç©ºé—´: å‡å°‘90%+",
        f"- è¿è¡Œæ—¶é—´: {total_time/60:.1f}åˆ†é’Ÿ (vs é¢„ä¼°3-12å°æ—¶)",
        f"- æ ·æœ¬è§„æ¨¡: {len(X_balanced):,} (ä¼˜åŒ–å)",
        "",
        "ğŸ“ˆ æ¨¡å‹å¯¹æ¯”:"
    ]

    for name, result in all_models.items():
        test_pred = result['model'].predict(X_test)
        test_acc = accuracy_score(y_test, test_pred)
        report_lines.append(f"- {name}: CV={result['best_score']:.3f}, Test={test_acc:.3f}")

    report_lines.extend([
        "",
        "ğŸ’¡ ä¸‹ä¸€æ­¥å»ºè®®:",
        "- å¦‚å·²è¾¾æ ‡: éƒ¨ç½²æ¨¡å‹è¿›è¡Œå®ç›˜æµ‹è¯•",
        "- å¦‚æœªè¾¾æ ‡: æ”¶é›†æ›´å¤šæ•°æ®æˆ–å°è¯•æ·±åº¦å­¦ä¹ ",
        "",
        "=" * 60
    ])

    # ä¿å­˜æŠ¥å‘Š
    report_content = "\n".join(report_lines)
    report_file = f"quick_optimization_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt"

    with open(report_file, 'w', encoding='utf-8') as f:
        f.write(report_content)

    print(f"ğŸ“„ æŠ¥å‘Šå·²ä¿å­˜: {report_file}")

    # ä¿å­˜æœ€ä½³æ¨¡å‹
    if success:
        model_file = f"best_model_quick_{best_model_name}_{datetime.now().strftime('%Y%m%d')}.pkl"

        model_package = {
            'model': best_model['model'],
            'feature_names': selected_features,
            'model_name': best_model_name,
            'test_accuracy': test_accuracy,
            'optimization_time': total_time
        }

        with open(model_file, 'wb') as f:
            pickle.dump(model_package, f)

        print(f"ğŸ’¾ æœ€ä½³æ¨¡å‹å·²ä¿å­˜: {model_file}")

    return {
        'best_model': best_model,
        'test_accuracy': test_accuracy,
        'optimization_time': total_time,
        'success': success
    }

if __name__ == "__main__":
    try:
        results = run_quick_optimization()

        print(f"\nğŸ‰ å¿«é€Ÿä¼˜åŒ–å®Œæˆï¼")
        print(f"â±ï¸ ç”¨æ—¶: {results['optimization_time']:.1f}ç§’")
        print(f"ğŸ“Š å‡†ç¡®ç‡: {results['test_accuracy']:.1%}")
        print(f"ğŸ¯ çŠ¶æ€: {'âœ… æˆåŠŸ' if results['success'] else 'âš ï¸ å¾…æ”¹è¿›'}")

    except Exception as e:
        print(f"âŒ å¿«é€Ÿä¼˜åŒ–å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()